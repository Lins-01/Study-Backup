{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: paddlenlp in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (2.5.2)\n",
      "Requirement already satisfied: typer in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.9.0)\n",
      "Requirement already satisfied: paddlefsl in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (1.1.0)\n",
      "Requirement already satisfied: visualdl in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (2.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.11.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.16.4)\n",
      "Requirement already satisfied: colorama in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.4.6)\n",
      "Requirement already satisfied: paddle2onnx in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (1.0.6)\n",
      "Requirement already satisfied: seqeval in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (2.13.1)\n",
      "Requirement already satisfied: fastapi in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.100.0)\n",
      "Requirement already satisfied: Flask-Babel<3.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (2.0.0)\n",
      "Requirement already satisfied: tqdm in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.1.99)\n",
      "Requirement already satisfied: colorlog in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (6.7.0)\n",
      "Requirement already satisfied: jieba in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied: dill<0.3.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.3.4)\n",
      "Requirement already satisfied: rich in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (13.4.2)\n",
      "Requirement already satisfied: uvicorn in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.22.0)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp) (0.70.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (3.8.5)\n",
      "Requirement already satisfied: xxhash in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (3.2.0)\n",
      "Requirement already satisfied: pandas in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (1.21.6)\n",
      "Requirement already satisfied: packaging in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (23.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (2023.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (2.31.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp) (6.7.0)\n",
      "Requirement already satisfied: Babel>=2.3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask-Babel<3.0.0->paddlenlp) (2.12.1)\n",
      "Requirement already satisfied: Flask in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask-Babel<3.0.0->paddlenlp) (2.2.5)\n",
      "Requirement already satisfied: Jinja2>=2.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask-Babel<3.0.0->paddlenlp) (3.1.2)\n",
      "Requirement already satisfied: pytz in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask-Babel<3.0.0->paddlenlp) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from huggingface-hub>=0.11.1->paddlenlp) (4.7.1)\n",
      "Requirement already satisfied: filelock in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from huggingface-hub>=0.11.1->paddlenlp) (3.12.2)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from fastapi->paddlenlp) (0.27.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from fastapi->paddlenlp) (2.1.1)\n",
      "Requirement already satisfied: six in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddle2onnx->paddlenlp) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from rich->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from rich->paddlenlp) (2.15.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from seqeval->paddlenlp) (1.0.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from typer->paddlenlp) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from uvicorn->paddlenlp) (0.12.0)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp) (3.5.3)\n",
      "Requirement already satisfied: bce-python-sdk in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp) (0.8.87)\n",
      "Requirement already satisfied: protobuf>=3.11.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp) (3.20.0)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask->Flask-Babel<3.0.0->paddlenlp) (2.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask->Flask-Babel<3.0.0->paddlenlp) (2.2.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (23.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (6.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Jinja2>=2.5->Flask-Babel<3.0.0->paddlenlp) (2.1.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->paddlenlp) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->paddlenlp) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->paddlenlp) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (3.4)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.7.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi->paddlenlp) (3.7.1)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.18.0)\n",
      "Requirement already satisfied: future>=0.6.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp) (3.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->paddlenlp) (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '楼面经理服务态度极差，等位和埋单都差，楼面小妹还挺好', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "\n",
    "from utils import convert_example\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "def read(data_path):\n",
    "    with open(data_path,'r',encoding='utf-8') as f:\n",
    "        for line in f :\n",
    "            l = line.strip('\\n').split('\\t')\n",
    "            if len(l) != 2:\n",
    "                print(len(l),line)\n",
    "            # python里对字符串对象操作，一般是拷贝操作，原来的不变\n",
    "            words ,label = line.strip('\\n').split('\\t')\n",
    "            yield {'text':words,'label':label}\n",
    "\n",
    "# lazy = True -> IterDataset ,不一次加载进内存 -> 访问不用下标，不用__getitem__ , for循环直接访问\n",
    "# False -> MapDataset : 在绝大多数时候都可以满足要求。一般只有在数据集过于庞大无法一次性加载进内存的时候我们才考虑使用 IterDataset \n",
    "train_ds = load_dataset(read,data_path='train.txt',lazy=False)\n",
    "dev_ds = load_dataset(read,data_path='dev.txt',lazy=False)\n",
    "test_ds = load_dataset(read,data_path='test.txt',lazy=False)\n",
    "print(test_ds[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapDataset的加载方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '赢在心理，输在出品！杨枝太酸，三文鱼熟了，酥皮焗杏汁杂果可以换个名（九唔搭八）', 'label': '0'}\n",
      "{'text': '服务一般，客人多，服务员少，但食品很不错', 'label': '1'}\n",
      "{'text': '東坡肉竟然有好多毛，問佢地點解，佢地仲話係咁架\\ue107\\ue107\\ue107\\ue107\\ue107\\ue107\\ue107冇天理，第一次食東坡肉有毛，波羅包就幾好食', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterDataset的加载方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '赢在心理，输在出品！杨枝太酸，三文鱼熟了，酥皮焗杏汁杂果可以换个名（九唔搭八）', 'label': '0'}\n",
      "{'text': '服务一般，客人多，服务员少，但食品很不错', 'label': '1'}\n",
      "{'text': '東坡肉竟然有好多毛，問佢地點解，佢地仲話係咁架\\ue107\\ue107\\ue107\\ue107\\ue107\\ue107\\ue107冇天理，第一次食東坡肉有毛，波羅包就幾好食', 'label': '0'}\n",
      "{'text': '父亲节去的，人很多，口味还可以上菜快！但是结账的时候，算错了没有打折，我也忘记拿清单了。说好打8折的，收银员没有打，人太多一时自己也没有想起。不知道收银员忘记，还是故意那钱露入自己钱包。。', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "train_ds_iter = load_dataset(read,data_path='train.txt',lazy=True)\n",
    "# 不用enumerate也可以\n",
    "# for item in train_ds_iter:\n",
    "for i,item in enumerate(train_ds_iter):\n",
    "    if(i>3):\n",
    "        break\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下载词汇表文件word_dict.txt，用于构造词-id映射关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt\n",
    "# windows用这个没下载下来\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词表\n",
    "vocab = Vocab.load_vocabulary('./senta_word_dict.txt',unk_token = '[UNK]',pad_token='[PAD]')\n",
    "\n",
    "# 使用词表初始化tokenizer\n",
    "tokenizer = JiebaTokenizer(vocab)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================test_ds.map(trans_function)\n",
      "==============================test_ds[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 440695,  764333, 1035593,  371677, 1106339,  995733,  237834,\n",
       "         891203,  258291, 1106339,  440695,  117037,  936761], dtype=int64),\n",
       " array(13, dtype=int64),\n",
       " array(0, dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_function = partial(\n",
    "    convert_example,\n",
    "    tokenizer = tokenizer,\n",
    "    is_test=False\n",
    ")\n",
    "print(\"=\"*30+'test_ds.map(trans_function)')\n",
    "test_ds.map(trans_function)\n",
    "print(\"=\"*30+'test_ds[0]')\n",
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(25, dtype=int64),\n",
       " array([ 656582,  967208,  318502, 1106339,       1,  693836, 1106328,\n",
       "         728300,   34934, 1106339,  677464, 1168226,  823066, 1106339,\n",
       "         706897, 1078813,  895713,   76982,  660347,       1,  179592,\n",
       "        1106335,  554600,       1, 1106336], dtype=int64),\n",
       " array(1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids=[656582, 967208, 318502, 1106339, 1, 693836, 1106328, 728300, 34934, 1106339, 677464, 1168226, 823066, 1106339, 706897, 1078813, 895713, 76982, 660347, 1, 179592, 1106335, 554600, 1, 1106336]\n",
    "valid_length = np.array(len(input_ids), dtype='int64')\n",
    "v = np.array(1)\n",
    "input_ids = np.array(input_ids, dtype='int64')\n",
    "valid_length,input_ids,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[128, 369], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [[656582 , 967208 , 318502 , ..., 0      , 0      , 0      ],\n",
      "        [724601 , 1250380, 1106339, ..., 0      , 0      , 0      ],\n",
      "        [283829 , 250030 , 389886 , ..., 0      , 0      , 0      ],\n",
      "        ...,\n",
      "        [278377 , 364676 , 952595 , ..., 0      , 0      , 0      ],\n",
      "        [137984 , 38435  , 399775 , ..., 0      , 0      , 0      ],\n",
      "        [115700 , 364716 , 509081 , ..., 0      , 0      , 0      ]]), Tensor(shape=[128], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [25 , 12 , 33 , 57 , 21 , 9  , 65 , 22 , 10 , 9  , 9  , 75 , 9  , 13 ,\n",
      "        244, 10 , 71 , 9  , 10 , 86 , 30 , 153, 15 , 15 , 21 , 30 , 23 , 98 ,\n",
      "        13 , 59 , 17 , 18 , 17 , 69 , 116, 192, 16 , 13 , 28 , 204, 28 , 10 ,\n",
      "        40 , 63 , 369, 12 , 10 , 58 , 15 , 11 , 18 , 32 , 130, 37 , 121, 271,\n",
      "        35 , 9  , 22 , 40 , 12 , 13 , 175, 23 , 6  , 39 , 32 , 18 , 15 , 8  ,\n",
      "        25 , 11 , 62 , 142, 11 , 10 , 19 , 14 , 22 , 96 , 12 , 8  , 11 , 63 ,\n",
      "        44 , 15 , 294, 54 , 76 , 11 , 261, 35 , 91 , 11 , 9  , 10 , 22 , 48 ,\n",
      "        19 , 10 , 192, 20 , 20 , 15 , 11 , 96 , 36 , 30 , 8  , 57 , 10 , 26 ,\n",
      "        20 , 80 , 15 , 11 , 11 , 17 , 20 , 23 , 12 , 25 , 10 , 24 , 13 , 105,\n",
      "        35 , 11 ]), Tensor(shape=[128], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,\n",
      "       [0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "# 读入数据，生成mini-batches\n",
    "def create_dataloader(dataset,\n",
    "                      trans_function=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      pad_token_id=0,\n",
    "                      batchify_fn=None):\n",
    "    # 下面在定义\n",
    "    if trans_function:\n",
    "        # 每个样本都应用trans_function里的函数\n",
    "        dataset_map = dataset.map(trans_function)\n",
    "    \n",
    "    # return_list 数据是否以list形式返回\n",
    "    # collate_fn  指定如何将样本列表组合为mini-batch数据。\n",
    "    # 传给它参数需要是一个callable对象，需要实现对组建的batch的处理逻辑，并返回每个batch的数据。\n",
    "    # 在这里传入的是`prepare_input`函数，对产生的数据进行pad操作，并返回实际长度等。\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset_map, # 从这个参数加载数据集\n",
    "        return_list = True,\n",
    "        batch_size = batch_size,\n",
    "        collate_fn=batchify_fn\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "#  python中的偏函数partial，把一个函数的某些参数固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。\n",
    "trans_function = partial(\n",
    "    convert_example,\n",
    "    tokenizer = tokenizer,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "# 将读入的数据batch化处理，便于模型batch化运算。\n",
    "# batch中的每个句子将会padding到这个batch中的文本最大长度batch_max_seq_len。\n",
    "# 当文本长度大于batch_max_seq时，将会截断到batch_max_seq_len；当文本长度小于batch_max_seq时，将会padding补齐到batch_max_seq_len.\n",
    "\n",
    "# ：号前是参数，后面是表达式，返回表达式的返回值\n",
    "# lambda a,b:a+b  \n",
    "batchify_fn =  lambda samples,fn=Tuple(\n",
    "    Pad(axis=0,pad_val=vocab['[PAD]']), # input_id\n",
    "    Stack(dtype='int64'), # seq_len\n",
    "    Stack(dtype='int64') # label\n",
    "):[data for data in fn (samples)]\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    trans_function=trans_function,\n",
    "    batch_size=128,\n",
    "    mode='train',\n",
    "    batchify_fn=batchify_fn\n",
    ")\n",
    "dev_loader =create_dataloader(\n",
    "    dev_ds,\n",
    "    trans_function=trans_function,\n",
    "    batch_size=128,\n",
    "    mode='validation',\n",
    "    batchify_fn=batchify_fn\n",
    ")\n",
    "test_loader =create_dataloader(\n",
    "    test_ds,\n",
    "    trans_function=trans_function,\n",
    "    batch_size=128,\n",
    "    mode='test',\n",
    "    batchify_fn=batchify_fn\n",
    ")\n",
    "\n",
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型搭建\n",
    "使用`LSTMencoder`搭建一个BiLSTM模型用于进行句子建模，得到句子的向量表示。\n",
    "\n",
    "然后接一个线性变换层，完成二分类任务。\n",
    "\n",
    "- `paddle.nn.Embedding`组建word-embedding层\n",
    "- `ppnlp.seq2vec.LSTMEncoder`组建句子建模层\n",
    "- `paddle.nn.Linear`构造二分类器\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/ecf309c20e5347399c55f1e067821daa088842fa46ad49be90de4933753cd3cf\" width = \"800\" height = \"450\"  hspace='10'/> <br />\n",
    "</p><br><center>图1：seq2vec示意图</center></br>\n",
    "\n",
    "* 除LSTM外，`seq2vec`还提供了许多语义表征方法，详细可参考：[seq2vec介绍]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "\n",
    "        # 首先将输入word id 查表后映射成 word embedding\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        # 将word embedding经过LSTMEncoder变换到文本语义表征空间中\n",
    "        self.lstm_encoder = ppnlp.seq2vec.LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "\n",
    "        # LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "\n",
    "        # 最后的分类器\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        # text shape: (batch_size, num_tokens)\n",
    "        # print('input :', text.shape)\n",
    "        \n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # print('after word-embeding:', embedded_text.shape)\n",
    "\n",
    "        # Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)\n",
    "        # num_directions = 2 if direction is 'bidirectional' else 1\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "        # print('after lstm:', text_repr.shape)\n",
    "\n",
    "\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "        # print('after Linear classifier:', fc_out.shape)\n",
    "\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        # print('output:', logits.shape)\n",
    "        \n",
    "        # probs 分类概率值\n",
    "        probs = F.softmax(logits, axis=-1)\n",
    "        # print('output probability:', probs.shape)\n",
    "        return probs\n",
    "\n",
    "model= LSTMModel(\n",
    "        len(vocab),\n",
    "        2,\n",
    "        direction='bidirectional',\n",
    "        padding_idx=vocab['[PAD]'])\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型配置和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(\n",
    "        parameters=model.parameters(), learning_rate=5e-5)\n",
    "\n",
    "loss = paddle.nn.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "model.prepare(optimizer, loss, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置visualdl路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置visualdl路径\n",
    "log_dir = './visualdl'\n",
    "callback = paddle.callbacks.VisualDL(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/10\n",
      "step  10/125 - loss: 0.6931 - acc: 0.4648 - 356ms/step\n",
      "step  20/125 - loss: 0.6931 - acc: 0.4773 - 234ms/step\n",
      "step  30/125 - loss: 0.6925 - acc: 0.4974 - 193ms/step\n",
      "step  40/125 - loss: 0.6916 - acc: 0.5039 - 171ms/step\n",
      "step  50/125 - loss: 0.6893 - acc: 0.5091 - 160ms/step\n",
      "step  60/125 - loss: 0.6939 - acc: 0.5091 - 151ms/step\n",
      "step  70/125 - loss: 0.6927 - acc: 0.5104 - 145ms/step\n",
      "step  80/125 - loss: 0.6890 - acc: 0.5092 - 141ms/step\n",
      "step  90/125 - loss: 0.6923 - acc: 0.5092 - 138ms/step\n",
      "step 100/125 - loss: 0.6874 - acc: 0.5106 - 135ms/step\n",
      "step 110/125 - loss: 0.6845 - acc: 0.5105 - 133ms/step\n",
      "step 120/125 - loss: 0.6869 - acc: 0.5096 - 131ms/step\n",
      "step 125/125 - loss: 0.6844 - acc: 0.5120 - 128ms/step\n",
      "save checkpoint at e:\\Document\\CodeSpace\\Study\\DeepL\\Process_data\\checkpoints\\0\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.6854 - acc: 0.6367 - 96ms/step\n",
      "step 20/84 - loss: 0.6841 - acc: 0.6352 - 83ms/step\n",
      "step 30/84 - loss: 0.6844 - acc: 0.6318 - 79ms/step\n",
      "step 40/84 - loss: 0.6837 - acc: 0.6309 - 77ms/step\n",
      "step 50/84 - loss: 0.6850 - acc: 0.6328 - 76ms/step\n",
      "step 60/84 - loss: 0.6801 - acc: 0.6358 - 75ms/step\n",
      "step 70/84 - loss: 0.6843 - acc: 0.6337 - 74ms/step\n",
      "step 80/84 - loss: 0.6835 - acc: 0.6332 - 72ms/step\n",
      "step 84/84 - loss: 0.6902 - acc: 0.6347 - 70ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 2/10\n",
      "step  10/125 - loss: 0.6829 - acc: 0.7227 - 140ms/step\n",
      "step  20/125 - loss: 0.6796 - acc: 0.7426 - 131ms/step\n",
      "step  30/125 - loss: 0.6749 - acc: 0.7529 - 128ms/step\n",
      "step  40/125 - loss: 0.6659 - acc: 0.7488 - 123ms/step\n",
      "step  50/125 - loss: 0.6538 - acc: 0.7394 - 122ms/step\n",
      "step  60/125 - loss: 0.6392 - acc: 0.7432 - 120ms/step\n",
      "step  70/125 - loss: 0.6089 - acc: 0.7561 - 120ms/step\n",
      "step  80/125 - loss: 0.5515 - acc: 0.7631 - 120ms/step\n",
      "step  90/125 - loss: 0.4700 - acc: 0.7717 - 120ms/step\n",
      "step 100/125 - loss: 0.4260 - acc: 0.7776 - 119ms/step\n",
      "step 110/125 - loss: 0.4521 - acc: 0.7862 - 119ms/step\n",
      "step 120/125 - loss: 0.4477 - acc: 0.7941 - 119ms/step\n",
      "step 125/125 - loss: 0.4417 - acc: 0.7967 - 117ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.4379 - acc: 0.8805 - 99ms/step\n",
      "step 20/84 - loss: 0.4101 - acc: 0.8828 - 86ms/step\n",
      "step 30/84 - loss: 0.4858 - acc: 0.8865 - 85ms/step\n",
      "step 40/84 - loss: 0.4181 - acc: 0.8877 - 83ms/step\n",
      "step 50/84 - loss: 0.4307 - acc: 0.8883 - 81ms/step\n",
      "step 60/84 - loss: 0.4481 - acc: 0.8857 - 80ms/step\n",
      "step 70/84 - loss: 0.4554 - acc: 0.8859 - 79ms/step\n",
      "step 80/84 - loss: 0.4299 - acc: 0.8861 - 78ms/step\n",
      "step 84/84 - loss: 0.4922 - acc: 0.8864 - 75ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 3/10\n",
      "step  10/125 - loss: 0.4942 - acc: 0.8734 - 144ms/step\n",
      "step  20/125 - loss: 0.4490 - acc: 0.8852 - 128ms/step\n",
      "step  30/125 - loss: 0.4681 - acc: 0.8878 - 122ms/step\n",
      "step  40/125 - loss: 0.4168 - acc: 0.8908 - 118ms/step\n",
      "step  50/125 - loss: 0.3994 - acc: 0.8970 - 116ms/step\n",
      "step  60/125 - loss: 0.4021 - acc: 0.9026 - 115ms/step\n",
      "step  70/125 - loss: 0.3825 - acc: 0.9058 - 114ms/step\n",
      "step  80/125 - loss: 0.3945 - acc: 0.9085 - 114ms/step\n",
      "step  90/125 - loss: 0.3706 - acc: 0.9109 - 114ms/step\n",
      "step 100/125 - loss: 0.3774 - acc: 0.9120 - 114ms/step\n",
      "step 110/125 - loss: 0.3814 - acc: 0.9133 - 114ms/step\n",
      "step 120/125 - loss: 0.4023 - acc: 0.9149 - 113ms/step\n",
      "step 125/125 - loss: 0.4107 - acc: 0.9149 - 111ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.4089 - acc: 0.9164 - 93ms/step\n",
      "step 20/84 - loss: 0.3776 - acc: 0.9227 - 80ms/step\n",
      "step 30/84 - loss: 0.4378 - acc: 0.9263 - 76ms/step\n",
      "step 40/84 - loss: 0.3768 - acc: 0.9281 - 74ms/step\n",
      "step 50/84 - loss: 0.3915 - acc: 0.9286 - 73ms/step\n",
      "step 60/84 - loss: 0.3876 - acc: 0.9286 - 72ms/step\n",
      "step 70/84 - loss: 0.4070 - acc: 0.9285 - 71ms/step\n",
      "step 80/84 - loss: 0.3989 - acc: 0.9285 - 70ms/step\n",
      "step 84/84 - loss: 0.4620 - acc: 0.9292 - 67ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 4/10\n",
      "step  10/125 - loss: 0.3969 - acc: 0.9250 - 129ms/step\n",
      "step  20/125 - loss: 0.4008 - acc: 0.9297 - 118ms/step\n",
      "step  30/125 - loss: 0.4117 - acc: 0.9289 - 114ms/step\n",
      "step  40/125 - loss: 0.3951 - acc: 0.9311 - 111ms/step\n",
      "step  50/125 - loss: 0.3771 - acc: 0.9344 - 111ms/step\n",
      "step  60/125 - loss: 0.3617 - acc: 0.9391 - 109ms/step\n",
      "step  70/125 - loss: 0.3508 - acc: 0.9423 - 109ms/step\n",
      "step  80/125 - loss: 0.3613 - acc: 0.9431 - 109ms/step\n",
      "step  90/125 - loss: 0.3508 - acc: 0.9439 - 109ms/step\n",
      "step 100/125 - loss: 0.3532 - acc: 0.9438 - 109ms/step\n",
      "step 110/125 - loss: 0.3702 - acc: 0.9442 - 109ms/step\n",
      "step 120/125 - loss: 0.3847 - acc: 0.9453 - 109ms/step\n",
      "step 125/125 - loss: 0.3852 - acc: 0.9450 - 107ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3863 - acc: 0.9375 - 92ms/step\n",
      "step 20/84 - loss: 0.3681 - acc: 0.9434 - 79ms/step\n",
      "step 30/84 - loss: 0.3884 - acc: 0.9422 - 76ms/step\n",
      "step 40/84 - loss: 0.3658 - acc: 0.9447 - 74ms/step\n",
      "step 50/84 - loss: 0.3889 - acc: 0.9448 - 73ms/step\n",
      "step 60/84 - loss: 0.3586 - acc: 0.9452 - 72ms/step\n",
      "step 70/84 - loss: 0.3832 - acc: 0.9445 - 72ms/step\n",
      "step 80/84 - loss: 0.3926 - acc: 0.9443 - 70ms/step\n",
      "step 84/84 - loss: 0.4483 - acc: 0.9451 - 67ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 5/10\n",
      "step  10/125 - loss: 0.3600 - acc: 0.9453 - 129ms/step\n",
      "step  20/125 - loss: 0.3834 - acc: 0.9477 - 118ms/step\n",
      "step  30/125 - loss: 0.3847 - acc: 0.9458 - 114ms/step\n",
      "step  40/125 - loss: 0.3810 - acc: 0.9473 - 111ms/step\n",
      "step  50/125 - loss: 0.3561 - acc: 0.9503 - 110ms/step\n",
      "step  60/125 - loss: 0.3433 - acc: 0.9531 - 109ms/step\n",
      "step  70/125 - loss: 0.3385 - acc: 0.9558 - 109ms/step\n",
      "step  80/125 - loss: 0.3456 - acc: 0.9564 - 109ms/step\n",
      "step  90/125 - loss: 0.3381 - acc: 0.9569 - 109ms/step\n",
      "step 100/125 - loss: 0.3405 - acc: 0.9570 - 109ms/step\n",
      "step 110/125 - loss: 0.3606 - acc: 0.9572 - 109ms/step\n",
      "step 120/125 - loss: 0.3776 - acc: 0.9578 - 109ms/step\n",
      "step 125/125 - loss: 0.3646 - acc: 0.9573 - 107ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3688 - acc: 0.9531 - 92ms/step\n",
      "step 20/84 - loss: 0.3585 - acc: 0.9563 - 80ms/step\n",
      "step 30/84 - loss: 0.3737 - acc: 0.9557 - 76ms/step\n",
      "step 40/84 - loss: 0.3512 - acc: 0.9557 - 74ms/step\n",
      "step 50/84 - loss: 0.3710 - acc: 0.9563 - 73ms/step\n",
      "step 60/84 - loss: 0.3540 - acc: 0.9557 - 73ms/step\n",
      "step 70/84 - loss: 0.3749 - acc: 0.9545 - 72ms/step\n",
      "step 80/84 - loss: 0.3712 - acc: 0.9543 - 70ms/step\n",
      "step 84/84 - loss: 0.4428 - acc: 0.9546 - 68ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 6/10\n",
      "step  10/125 - loss: 0.3556 - acc: 0.9477 - 131ms/step\n",
      "step  20/125 - loss: 0.3718 - acc: 0.9520 - 119ms/step\n",
      "step  30/125 - loss: 0.3608 - acc: 0.9547 - 114ms/step\n",
      "step  40/125 - loss: 0.3629 - acc: 0.9563 - 111ms/step\n",
      "step  50/125 - loss: 0.3510 - acc: 0.9587 - 112ms/step\n",
      "step  60/125 - loss: 0.3459 - acc: 0.9617 - 111ms/step\n",
      "step  70/125 - loss: 0.3308 - acc: 0.9637 - 111ms/step\n",
      "step  80/125 - loss: 0.3408 - acc: 0.9642 - 111ms/step\n",
      "step  90/125 - loss: 0.3313 - acc: 0.9643 - 111ms/step\n",
      "step 100/125 - loss: 0.3712 - acc: 0.9614 - 111ms/step\n",
      "step 110/125 - loss: 0.3707 - acc: 0.9597 - 111ms/step\n",
      "step 120/125 - loss: 0.3881 - acc: 0.9600 - 111ms/step\n",
      "step 125/125 - loss: 0.3572 - acc: 0.9598 - 109ms/step\n",
      "save checkpoint at e:\\Document\\CodeSpace\\Study\\DeepL\\Process_data\\checkpoints\\5\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3729 - acc: 0.9484 - 100ms/step\n",
      "step 20/84 - loss: 0.3617 - acc: 0.9551 - 86ms/step\n",
      "step 30/84 - loss: 0.3846 - acc: 0.9555 - 81ms/step\n",
      "step 40/84 - loss: 0.3482 - acc: 0.9572 - 79ms/step\n",
      "step 50/84 - loss: 0.3666 - acc: 0.9572 - 78ms/step\n",
      "step 60/84 - loss: 0.3389 - acc: 0.9576 - 77ms/step\n",
      "step 70/84 - loss: 0.3888 - acc: 0.9563 - 77ms/step\n",
      "step 80/84 - loss: 0.3717 - acc: 0.9563 - 75ms/step\n",
      "step 84/84 - loss: 0.3468 - acc: 0.9569 - 73ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 7/10\n",
      "step  10/125 - loss: 0.3461 - acc: 0.9555 - 137ms/step\n",
      "step  20/125 - loss: 0.3571 - acc: 0.9609 - 132ms/step\n",
      "step  30/125 - loss: 0.3475 - acc: 0.9609 - 127ms/step\n",
      "step  40/125 - loss: 0.3577 - acc: 0.9617 - 125ms/step\n",
      "step  50/125 - loss: 0.3461 - acc: 0.9637 - 124ms/step\n",
      "step  60/125 - loss: 0.3484 - acc: 0.9625 - 121ms/step\n",
      "step  70/125 - loss: 0.3715 - acc: 0.9569 - 119ms/step\n",
      "step  80/125 - loss: 0.3439 - acc: 0.9557 - 117ms/step\n",
      "step  90/125 - loss: 0.3266 - acc: 0.9572 - 117ms/step\n",
      "step 100/125 - loss: 0.3317 - acc: 0.9580 - 116ms/step\n",
      "step 110/125 - loss: 0.3503 - acc: 0.9587 - 116ms/step\n",
      "step 120/125 - loss: 0.3757 - acc: 0.9597 - 115ms/step\n",
      "step 125/125 - loss: 0.3549 - acc: 0.9597 - 113ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3620 - acc: 0.9539 - 93ms/step\n",
      "step 20/84 - loss: 0.3529 - acc: 0.9594 - 81ms/step\n",
      "step 30/84 - loss: 0.3626 - acc: 0.9596 - 79ms/step\n",
      "step 40/84 - loss: 0.3456 - acc: 0.9602 - 77ms/step\n",
      "step 50/84 - loss: 0.3707 - acc: 0.9595 - 76ms/step\n",
      "step 60/84 - loss: 0.3374 - acc: 0.9592 - 75ms/step\n",
      "step 70/84 - loss: 0.3793 - acc: 0.9583 - 73ms/step\n",
      "step 80/84 - loss: 0.3698 - acc: 0.9576 - 72ms/step\n",
      "step 84/84 - loss: 0.3281 - acc: 0.9583 - 69ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 8/10\n",
      "step  10/125 - loss: 0.3400 - acc: 0.9563 - 133ms/step\n",
      "step  20/125 - loss: 0.3467 - acc: 0.9641 - 122ms/step\n",
      "step  30/125 - loss: 0.3437 - acc: 0.9651 - 118ms/step\n",
      "step  40/125 - loss: 0.3523 - acc: 0.9658 - 115ms/step\n",
      "step  50/125 - loss: 0.3424 - acc: 0.9678 - 114ms/step\n",
      "step  60/125 - loss: 0.3339 - acc: 0.9693 - 113ms/step\n",
      "step  70/125 - loss: 0.3229 - acc: 0.9710 - 113ms/step\n",
      "step  80/125 - loss: 0.3290 - acc: 0.9717 - 113ms/step\n",
      "step  90/125 - loss: 0.3243 - acc: 0.9719 - 113ms/step\n",
      "step 100/125 - loss: 0.3291 - acc: 0.9716 - 113ms/step\n",
      "step 110/125 - loss: 0.3442 - acc: 0.9717 - 113ms/step\n",
      "step 120/125 - loss: 0.3592 - acc: 0.9723 - 113ms/step\n",
      "step 125/125 - loss: 0.3458 - acc: 0.9721 - 111ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3596 - acc: 0.9586 - 94ms/step\n",
      "step 20/84 - loss: 0.3514 - acc: 0.9633 - 81ms/step\n",
      "step 30/84 - loss: 0.3583 - acc: 0.9635 - 78ms/step\n",
      "step 40/84 - loss: 0.3408 - acc: 0.9641 - 76ms/step\n",
      "step 50/84 - loss: 0.3644 - acc: 0.9636 - 76ms/step\n",
      "step 60/84 - loss: 0.3341 - acc: 0.9639 - 75ms/step\n",
      "step 70/84 - loss: 0.3730 - acc: 0.9624 - 74ms/step\n",
      "step 80/84 - loss: 0.3674 - acc: 0.9619 - 72ms/step\n",
      "step 84/84 - loss: 0.3245 - acc: 0.9624 - 70ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 9/10\n",
      "step  10/125 - loss: 0.3380 - acc: 0.9625 - 134ms/step\n",
      "step  20/125 - loss: 0.3325 - acc: 0.9699 - 125ms/step\n",
      "step  30/125 - loss: 0.3403 - acc: 0.9701 - 123ms/step\n",
      "step  40/125 - loss: 0.3510 - acc: 0.9699 - 122ms/step\n",
      "step  50/125 - loss: 0.3400 - acc: 0.9714 - 123ms/step\n",
      "step  60/125 - loss: 0.3320 - acc: 0.9725 - 122ms/step\n",
      "step  70/125 - loss: 0.3461 - acc: 0.9713 - 122ms/step\n",
      "step  80/125 - loss: 0.3780 - acc: 0.9663 - 121ms/step\n",
      "step  90/125 - loss: 0.3277 - acc: 0.9638 - 120ms/step\n",
      "step 100/125 - loss: 0.3358 - acc: 0.9629 - 120ms/step\n",
      "step 110/125 - loss: 0.3397 - acc: 0.9638 - 119ms/step\n",
      "step 120/125 - loss: 0.3641 - acc: 0.9651 - 119ms/step\n",
      "step 125/125 - loss: 0.3315 - acc: 0.9652 - 116ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3665 - acc: 0.9586 - 94ms/step\n",
      "step 20/84 - loss: 0.3536 - acc: 0.9629 - 81ms/step\n",
      "step 30/84 - loss: 0.3578 - acc: 0.9633 - 78ms/step\n",
      "step 40/84 - loss: 0.3415 - acc: 0.9648 - 75ms/step\n",
      "step 50/84 - loss: 0.3553 - acc: 0.9645 - 75ms/step\n",
      "step 60/84 - loss: 0.3318 - acc: 0.9650 - 74ms/step\n",
      "step 70/84 - loss: 0.3635 - acc: 0.9638 - 73ms/step\n",
      "step 80/84 - loss: 0.3747 - acc: 0.9629 - 72ms/step\n",
      "step 84/84 - loss: 0.3255 - acc: 0.9633 - 69ms/step\n",
      "Eval samples: 10644\n",
      "Epoch 10/10\n",
      "step  10/125 - loss: 0.3408 - acc: 0.9648 - 135ms/step\n",
      "step  20/125 - loss: 0.3283 - acc: 0.9723 - 125ms/step\n",
      "step  30/125 - loss: 0.3481 - acc: 0.9721 - 120ms/step\n",
      "step  40/125 - loss: 0.3514 - acc: 0.9729 - 117ms/step\n",
      "step  50/125 - loss: 0.3349 - acc: 0.9741 - 117ms/step\n",
      "step  60/125 - loss: 0.3274 - acc: 0.9754 - 115ms/step\n",
      "step  70/125 - loss: 0.3199 - acc: 0.9767 - 115ms/step\n",
      "step  80/125 - loss: 0.3278 - acc: 0.9770 - 114ms/step\n",
      "step  90/125 - loss: 0.3206 - acc: 0.9769 - 114ms/step\n",
      "step 100/125 - loss: 0.3250 - acc: 0.9768 - 115ms/step\n",
      "step 110/125 - loss: 0.3388 - acc: 0.9770 - 115ms/step\n",
      "step 120/125 - loss: 0.3600 - acc: 0.9775 - 114ms/step\n",
      "step 125/125 - loss: 0.3259 - acc: 0.9774 - 113ms/step\n",
      "Eval begin...\n",
      "step 10/84 - loss: 0.3611 - acc: 0.9586 - 97ms/step\n",
      "step 20/84 - loss: 0.3526 - acc: 0.9629 - 83ms/step\n",
      "step 30/84 - loss: 0.3572 - acc: 0.9635 - 79ms/step\n",
      "step 40/84 - loss: 0.3378 - acc: 0.9652 - 77ms/step\n",
      "step 50/84 - loss: 0.3616 - acc: 0.9652 - 76ms/step\n",
      "step 60/84 - loss: 0.3309 - acc: 0.9660 - 75ms/step\n",
      "step 70/84 - loss: 0.3587 - acc: 0.9653 - 74ms/step\n",
      "step 80/84 - loss: 0.3739 - acc: 0.9645 - 72ms/step\n",
      "step 84/84 - loss: 0.3276 - acc: 0.9648 - 69ms/step\n",
      "Eval samples: 10644\n",
      "save checkpoint at e:\\Document\\CodeSpace\\Study\\DeepL\\Process_data\\checkpoints\\final\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, dev_loader, epochs=10, save_dir='./checkpoints', save_freq=5, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "step 10/84 - loss: 0.3611 - acc: 0.9586 - 98ms/step\n",
      "step 20/84 - loss: 0.3526 - acc: 0.9629 - 85ms/step\n",
      "step 30/84 - loss: 0.3572 - acc: 0.9635 - 81ms/step\n",
      "step 40/84 - loss: 0.3378 - acc: 0.9652 - 78ms/step\n",
      "step 50/84 - loss: 0.3616 - acc: 0.9652 - 77ms/step\n",
      "step 60/84 - loss: 0.3309 - acc: 0.9660 - 76ms/step\n",
      "step 70/84 - loss: 0.3587 - acc: 0.9653 - 75ms/step\n",
      "step 80/84 - loss: 0.3739 - acc: 0.9645 - 73ms/step\n",
      "step 84/84 - loss: 0.3276 - acc: 0.9648 - 70ms/step\n",
      "Eval samples: 10644\n",
      "Finally test acc: 0.96477\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(dev_loader)\n",
    "print(\"Finally test acc: %.5f\" % results['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'paddlenlp.datasets.dataset.MapDataset'>\n",
      "Predict begin...\n",
      "step 42/42 [==============================] - 72ms/step          \n",
      "Predict samples: 5353\n"
     ]
    }
   ],
   "source": [
    "print(type(test_ds))\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "results = model.predict(test_loader, batch_size=128)[0]\n",
    "predictions = []\n",
    "\n",
    "for batch_probs in results:\n",
    "    # 映射分类label\n",
    "    idx = np.argmax(batch_probs, axis=-1)\n",
    "    idx = idx.tolist()\n",
    "    labels = [label_map[i] for i in idx]\n",
    "    predictions.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 440695,  764333, 1035593,  371677, 1106339,  995733,  237834,\n",
      "        891203,  258291, 1106339,  440695,  117037,  936761], dtype=int64), array(13, dtype=int64), array(0, dtype=int64))\n",
      "<class 'tuple'>\n",
      "Data: [ 440695  764333 1035593  371677 1106339  995733  237834  891203  258291\n",
      " 1106339  440695  117037  936761] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [ 471791  936825 1022922  761432  891564 1057229  549892  859353 1106326\n",
      "  653811  176187  877695  958129  173188  986608 1106339  781255  830165\n",
      "  213378  535515   36026 1106326 1106328   25529  749968 1106339  147848\n",
      "  830171  489131  958129 1106339  479899  930707  173188  399212       1\n",
      " 1222901  508478  823066 1106326 1106328  651025  869365       1  681075\n",
      " 1106339  453143  830172  790286 1051917  173188  681075  173401  412947\n",
      "  747344 1106326 1106328] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [ 451938  696658  748698  302748  936106  308649  157793  718272  660347\n",
      "   40882   86562  510099 1106339 1050713  321211   69882  686776  816540\n",
      " 1106339  178687 1106328 1057229  568351 1106328  606478  703976  823066\n",
      "  156991 1211275  364716 1092505  877695 1106339  979319 1106326 1175853\n",
      "    4783  267842  851077  639231 1106339  786065 1106339  795996  953794\n",
      "  179568  974589  974589       1  332002  803010  340371  668761    4783\n",
      "  620305       1 1106328  713424  830171   35393  898857  472344 1106339\n",
      "  157793   47104 1191873  718272  389733  738354  952595 1106339  306239\n",
      "  898857  616980 1106326 1106328 1106326 1106328  789340  696658  711103\n",
      "  859353  202379  472344  179582 1106326   26156 1093873  146708  530102\n",
      " 1106339  607173 1107855  823066  974589  974589 1245808       1  453139\n",
      " 1193342  940533 1050739 1106339 1135123  213058       1  179568] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [1086178  173188 1188905  650296  389733  175045  669226 1106339  669226\n",
      "  604466  836929   52172 1106339  311074  881196 1044637  690740       1\n",
      " 1106339  766694  453530  511894       1       1  173188  690740  224268\n",
      "  257857  240658 1106339  874019 1238094 1057229  220620  173188 1106339\n",
      "  770594  963026 1106339  727945  608990  515300  936106  237795  282921\n",
      "  282921  282921  282921  282921  282921] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [ 365921  932416  663024  388662 1106328 1106328 1106328 1083318  823066\n",
      " 1216915  819627  453139 1216915  960087  159950  173188  753452 1106328] \t Label: positive\n",
      "<class 'tuple'>\n",
      "Data: [ 384261  305803  173188 1183540  823116 1106339  847645  389733  563178\n",
      " 1053035  666970   81913  475331 1203115  991057 1106339  479740   88185\n",
      "  859353 1111556  347427  608798  881165  769807  305838  322845  218407\n",
      "  135208  128530 1106339  802038    4782  285713    4782  107509  553338\n",
      "  287319  859353 1042186  550667    4782  881173 1183540  823116  173188\n",
      " 1173300    4782  881173 1183540  769807  173188 1173300    4783] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [ 376180  639231   99052  894574 1106328  240449  137984 1057229  250355\n",
      "  173188  910748 1106339  418471  926077  799171 1106339  724601  261577\n",
      " 1032720 1106339  860985 1134938 1106339  820003  543370  894574 1106328\n",
      " 1227535] \t Label: positive\n",
      "<class 'tuple'>\n",
      "Data: [1066901  363693  979812 1173329  173188  514629 1106321 1000183 1083318\n",
      " 1096990  846772  818554  167992 1192261  173188  326984 1117423  832658\n",
      "  832658  491743 1106328 1173329 1106328 1106328 1106328 1066901  363693\n",
      " 1029799       1  173188  514629   68233 1106321  473329       1  510170\n",
      "  400234 1106339  957006  823153  930660  794154  183395  382678  500902\n",
      "  659913  105959  173188  726352 1106339  166181  473329  928384  453139\n",
      "    4777 1053978    4778       1 1106339  276243  224268  108987  632721\n",
      " 1022416       1 1084561       1 1106339  553340  444988  869494       1\n",
      " 1204878  651025       1  656594  696404   66941  742636  416380  832658\n",
      "  832658       1 1106328  191148       1 1106328 1106328  876258 1106328\n",
      " 1106328 1106328  386946  803200  278738  353572  400234  173188  709767\n",
      " 1106321 1066901  363693  396639  173188  514629 1106321  453971 1121397\n",
      "  522245 1211275  881196 1098550  823066 1106339  684489 1046376  881196\n",
      " 1106339  767021   69882  823066  768905       1 1152574 1106322       1\n",
      "  686770 1106328 1106328 1066901  363693  396639  173188  514629 1106321\n",
      "  991391  956395    4782  816520 1164625  312864    4782  816520 1110707\n",
      "  272690    4782  816520  465165       1  560962 1034586    4782  816520\n",
      "       1  173188  110645  444615 1232146  853920  938841  302512 1106322\n",
      "       1  244170 1106328 1106328 1155475  599615  890242  954040       1\n",
      "  242649  173188  298309 1057229  803636    4782  629586  101669  173188\n",
      "  298309 1106339  177893    4782       1  173188 1211275 1084488  602020\n",
      " 1093154       1  263194 1106328 1106328  242649  173188  298309 1057229\n",
      "  453148  881196  803636  173188 1071540 1106328 1106328 1106328] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [ 401981  738738  884754  881187  261577  237795 1106339 1085335  297759\n",
      "  820275 1106328  810400  884358 1106328  323771  518954  173188  230599\n",
      "    4782] \t Label: negative\n",
      "<class 'tuple'>\n",
      "Data: [ 711617 1150733  173188  504130  440821  977875  477776  823066 1106339\n",
      "  242888  898857  543054  364676    4783  178053 1057229  820275 1106339\n",
      " 1121661  663813  898857  254013   40261  173188  650322  179568    4783\n",
      "  860985  965100  894607  553772  759049  560962  231110  196627  261574\n",
      "  389733  830171  939335 1106339  312183  284408  453172 1006966  231110\n",
      "  173188  284408 1106339  830172  389733  261577 1225280  823066    4783\n",
      "    4783  724601 1116122    6284 1118059    4783] \t Label: negative\n"
     ]
    }
   ],
   "source": [
    "# 看看预测数据前5个样例分类结果\n",
    "for i in test_ds:\n",
    "    print(i)\n",
    "    break\n",
    "    \n",
    "for idx, data in enumerate(test_ds):\n",
    "    if idx < 10:\n",
    "        print(type(data))\n",
    "        print('Data: {} \\t Label: {}'.format(data[0], predictions[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
