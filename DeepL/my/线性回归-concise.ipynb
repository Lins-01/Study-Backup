{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# torch.utils.data是PyTorch中用来处理数据的工具包\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据集\n",
    "这里这个函数应该是d2l内部自己写好提供的函数\n",
    "\n",
    "跟刚才自己定义的那个一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "# 数据集保存在features和labels中\n",
    "features,labels =d2l.synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays,batch_size,is_train=True):\n",
    "    \"\"\"\"构造一个 pytorch数据迭代器\"\"\"\n",
    "    # TensorDataset内定义了传入参数要是 *tensors \n",
    "    # *tensors是非关键字参数，所以传入要加*号\n",
    "\n",
    "    # data.TensorDataset将多个特征和标签数据组合成一个PyTorch的数据集对象\n",
    "    # 返回一个对象\n",
    "    # 方便后续进行数据处理和批处理操作。\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    # DataLoader是一个迭代器，每次返回batch_size个样本\n",
    "    # 创建一个数据迭代器，用于在训练过程中按批次加载和处理数据。\n",
    "\n",
    "    # 这里用return一次返回全部的数据\n",
    "    # 用yeild一次返回一个数据，所以差不多啦\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "data_iter = load_array((features,labels),batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 验证迭代器\n",
    "  -  `iter()`函数将迭代器 `data_iter` 转换为一个迭代器对象\n",
    "  -  `next()`函数获取该迭代器的下一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.7617, -1.1056],\n",
       "         [ 0.2292, -0.3574],\n",
       "         [-0.2596, -0.0485],\n",
       "         [ 0.8269,  0.8693],\n",
       "         [-0.7845, -0.3831],\n",
       "         [-1.1202, -0.4330],\n",
       "         [-2.0147, -1.0008],\n",
       "         [ 0.5543, -0.0742],\n",
       "         [ 1.7024,  1.1714],\n",
       "         [-0.2592,  0.8678]]),\n",
       " tensor([[4.4314],\n",
       "         [5.8924],\n",
       "         [3.8338],\n",
       "         [2.8894],\n",
       "         [3.9179],\n",
       "         [3.4410],\n",
       "         [3.5808],\n",
       "         [5.5492],\n",
       "         [3.6270],\n",
       "         [0.7493]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "- `Sequential`类将多个层串联在一起。\n",
    "- 当给定输入数据时，`Sequential`实例将数据传入到第一层，\n",
    "- 然后将第一层的输出作为第二层的输入，以此类推。\n",
    "\n",
    "- 这里定义了一个全连接层"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在PyTorch中，全连接层在`Linear`类中定义。\n",
    "- 值得注意的是，我们将两个参数传递到`nn.Linear`中。\n",
    "- 第一个指定输入特征形状，即2\n",
    "- 第二个指定输出特征形状，输出特征形状为单个标量，因此为1。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn`是神经网络的缩写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "- 框架里一般都有预定义方法来初始化参数\n",
    "- 我们通过`net[0]`选择网络中的第一个图层，\n",
    "- 然后使用`weight.data`和`bias.data`方法访问参数。\n",
    "- 我们还可以使用替换方法`normal_`和`fill_`来重写参数值。\n",
    "  - 我们可以通过`_`结尾的方法将参数替换，从而初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal_()函数将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布\n",
    "net[0].weight.data.normal_(0,0.01)\n",
    "\n",
    "# 不加后面的下划线，报错'Tensor' object has no attribute 'normal'\n",
    "# net[0].weight.data.normal(0,0.01)\n",
    "\n",
    "# bias.data.fill_(0)将偏差参数清零\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "- 框架都写好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化算法\n",
    "- 我们要指定优化的参数\n",
    "  - 可通过`net.parameters()`从我们的模型中获得\n",
    "- 小批量随机梯度下降只需要设置`lr`值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer =torch.optim.SGD(net.parameters(),lr=0.03)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "基本一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000241\n",
      "tensor([[-0.0065, -0.0103]])\n",
      "tensor([-0.0338])\n",
      "epoch 2, loss 0.000107\n",
      "tensor([[-0.0027,  0.0009]])\n",
      "tensor([-0.0009])\n",
      "epoch 3, loss 0.000109\n",
      "tensor([[-0.0030,  0.0032]])\n",
      "tensor([-0.0057])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        # 梯度清零\n",
    "        trainer.zero_grad()\n",
    "        # 反向传播,这里不用.sum()，因为框架实现了\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(net(features),labels)\n",
    "    print(f\"epoch {epoch+1}, loss {l:f}\")\n",
    "    # 查看梯度\n",
    "    print(net[0].weight.grad)\n",
    "    print(net[0].bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "* 我们可以使用PyTorch的高级API更简洁地实现模型。\n",
    "* 在PyTorch中，`data`模块提供了数据处理工具，`nn`模块定义了大量的神经网络层和常见损失函数。\n",
    "* 我们可以通过`_`结尾的方法将参数替换，从而初始化参数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "- 如何访问线性回归的梯度？\n",
    "  - 见训练最后一行代码"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_pytorch",
   "language": "python",
   "name": "d2l_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
