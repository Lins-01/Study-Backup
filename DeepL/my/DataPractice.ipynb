{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据表示总结\n",
    "深度学习存储和操作数据的主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "         0.9000, 1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000, 1.6000, 1.7000,\n",
       "         1.8000, 1.9000, 2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000, 2.6000,\n",
       "         2.7000, 2.8000, 2.9000, 3.0000, 3.1000, 3.2000, 3.3000, 3.4000, 3.5000,\n",
       "         3.6000, 3.7000, 3.8000, 3.9000, 4.0000, 4.1000, 4.2000, 4.3000, 4.4000,\n",
       "         4.5000, 4.6000, 4.7000, 4.8000, 4.9000]),\n",
       " torch.return_types.sort(\n",
       " values=tensor([0.0312, 0.3327, 0.3450, 0.6332, 0.6561, 0.6762, 0.8970, 1.1050, 1.1958,\n",
       "         1.2029, 1.3604, 1.5666, 1.7388, 1.7557, 1.7642, 1.8252, 1.8283, 1.8693,\n",
       "         2.0594, 2.0691, 2.0864, 2.1265, 2.4785, 2.5887, 2.6014, 2.6035, 2.7072,\n",
       "         2.7536, 2.7881, 2.8083, 3.0951, 3.2491, 3.3284, 3.4669, 3.5274, 3.5744,\n",
       "         3.6498, 3.6524, 3.9045, 3.9410, 3.9782, 4.1387, 4.2345, 4.2829, 4.3259,\n",
       "         4.4919, 4.5255, 4.6256, 4.8803, 4.9936]),\n",
       " indices=tensor([36, 35, 43, 17, 22, 27, 25, 37, 28, 31,  4,  5, 24, 46, 21, 33, 23, 20,\n",
       "         32, 19, 48, 10, 18, 12,  1,  9,  6, 40, 45, 49, 15,  2, 44, 16, 26,  3,\n",
       "         41, 39, 29, 42,  8, 38, 13, 30, 14, 11, 34, 47,  7,  0])),\n",
       " tensor([0.0472, 0.1979, 0.3464, 0.7101, 0.8135, 1.0974, 1.1219, 1.1817, 1.3087,\n",
       "         1.3983, 1.4774, 1.5062, 1.6019, 1.9111, 2.0488, 2.1690, 2.1881, 2.2547,\n",
       "         2.2810, 2.3420, 2.3982, 2.4883, 2.5271, 2.6201, 2.8705, 3.0176, 3.0517,\n",
       "         3.0931, 3.2000, 3.2247, 3.3164, 3.7058, 3.8479, 3.8750, 3.9162, 3.9324,\n",
       "         4.0212, 4.0730, 4.1016, 4.1653, 4.3659, 4.4160, 4.4276, 4.4484, 4.6390,\n",
       "         4.7261, 4.7455, 4.8683, 4.9132, 4.9626]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12)\n",
    "y = torch.arange(0,5,0.1)\n",
    "n_train = 50  # 训练样本数\n",
    "x_train= torch.sort(torch.rand(n_train)*5)   # 排序后的训练样本\n",
    "y_train,_= torch.sort(torch.rand(n_train)*5)   # 排序后的训练样本\n",
    "# 为什么x_train和y_train不同？y_train后面的,_是什么用法？\n",
    "#  _是一个占位符，表示不需要这个值，但是需要占位，否则会报错\n",
    "\n",
    "x,y, x_train, y_train\n",
    "\n",
    "# torch.xxx来赋值都是一个tensor\n",
    "# x = torch.arange(12) //输入范围\n",
    "# torch.ones/zeros/randn()// 输入维度\n",
    "# torch.tensor([])//输入具体数值/python列表\n",
    "# 每个tensor，也有自己的方法。reshape, shape，numel之类\n",
    "# tensor之间，也可以按照元素来进行加减乘除/平方/求幂之类的运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取张量tensor中的元素个数，number element。一维时与shape一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(2,6)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape时候，行/列只用指定一个。因为知道总数，另一个可以自动算出来。\n",
    "**不指定的那个要填上-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#等价于如上\n",
    "x = x.reshape(-1,6)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(2,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]),\n",
       " tensor([[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#一个方括号一个维度，看第一个方括号到邻近的最后一个方括号有几个，就是几维\n",
    "#如下，最外层的方括号，表示2个内层的\n",
    "#次一层的表示3个内层的\n",
    "#再次一层表示4个内层的\n",
    "torch.zeros(2,3,4) ,torch.zeros((2,3,4))\n",
    "\n",
    "#这两种写法一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7024,  0.2065, -0.4582, -0.0308],\n",
       "        [ 0.2086,  1.9039, -0.4320,  0.5849],\n",
       "        [ 0.9331, -0.8656, -0.4887,  0.0512]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)\n",
    "#取随机值，应该时默认符合标准正态分布\n",
    "#标准正态分布又称为u分布，是以0为均值、以1为标准差的正态分布，  记为N（0，1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "#用python的列表来赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按元素操作\n",
    "x = torch.tensor([1.0,2,4,8])\n",
    "y = torch.tensor([2,2,2,2])\n",
    "x + y, x - y, x * y, x / y,x **y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按元素操作，还包括，求幂\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "#dim=0,按行拼接（默认），=1，按列拼接\n",
    "torch.cat((X,Y),dim=0),torch.cat((X,Y),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断两个张量，可以得到布尔值的结果，相同为True，不用为False\n",
    "X==Y\n",
    "#下面结果显示，x与y只有两个相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对张量所有元素进行求和\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1, 2]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape(3,1)\n",
    "b = torch.arange(3).reshape(1,3)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 广播机制，不同类型的矩阵相加，扩成同型矩阵\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切片和索引，切片就是另一种选取元素的方式\n",
    "X = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
    "X,X[-1],X[1:3]\n",
    "#-1选最后一个元素，因为是二维的，所以一个元素就是一个向量\n",
    "#1:3选第二个和第三个元素\n",
    "#感觉是不加 , 逗号 ，就默认是第一个维度，这里就是默认行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 99.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可对单个元素赋值\n",
    "# 对第2行第3列的元素进行赋值\n",
    "X[1,2]=99\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[77., 77., 77., 77.],\n",
       "        [77., 77., 77., 77.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选中多行 多列可批量赋值\n",
    "# 如下选中第一行和第二行，0：2,不包括第三行，相当于右边开区间\n",
    "# 逗号分隔一个维度，逗号后面， ： ,此符号表示所有列\n",
    "X[0:2,:]=77\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 节省点内存\n",
    "# 因为深度学习跑模型每秒运算太大了，或许还有别的引用指向原内存\n",
    "\n",
    "#起因\n",
    "#因为python中如下写运算，Y会重新得到个内存\n",
    "#id()，来获取y的地址\n",
    "y=1\n",
    "x=2\n",
    "before=id(Y)\n",
    "Y=X+Y\n",
    "id(Y)==before\n",
    "#会输出false，说明地址变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#向下面这样写就不会变\n",
    "before=id(Y)\n",
    "Y[:]=X+Y\n",
    "id(Y)==before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这样也行\n",
    "Y+=X\n",
    "id(Y)==before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个相似结构的tensor\n",
    "# _like 表示和Y结构一样\n",
    "# zeros表示元素为0\n",
    "Z=torch.zeros_like(Y)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " torch.Tensor,\n",
       " array([[77., 77., 77., 77.],\n",
       "        [77., 77., 77., 77.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=float32),\n",
       " tensor([[77., 77., 77., 77.],\n",
       "         [77., 77., 77., 77.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([77., 77., 10.]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch的张量(tensor)转换为Numpy的张量(ndarry),很容易，反之也很容易\n",
    "#如下，将ndarry->tensor\n",
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A),type(B),A,B,B[:,2]\n",
    "# 切片表示具体某一列时，是直接按索引号来的\n",
    "# 所以B[:,2]取最后一列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将大小为1的张量转换为python的标量\n",
    "a = torch.tensor([3.5])\n",
    "# a.item(),将a转为标量 , 后面两个就用的python的强转\n",
    "a,a.item(),float(a),int(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy矩阵运算（求逆、行列式值、特征向量特征值、求解线性方程组）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -2. ],\n",
       "       [-0.5,  1.5]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A=np.array([\n",
    "    [3,4],\n",
    "    [1,2]\n",
    "])\n",
    "#求逆矩阵\n",
    "a=np.linalg.inv(A)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000000000000004"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求行列式\n",
    "a=np.linalg.det(A)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.56155281, 0.43844719]),\n",
       " array([[ 0.93153209, -0.84212294],\n",
       "        [ 0.36365914,  0.5392856 ]]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求特征值和特征向量\n",
    "a=np.linalg.eig(A)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7. ,  7.5])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#线性方程组Ax=B求解\n",
    "B=np.array([9,8])\n",
    "x=np.linalg.solve(A,B)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3, 4],\n",
       "        [1, 2]]),\n",
       " array([[3, 1],\n",
       "        [4, 2]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵转置\n",
    "A,A.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### help文档使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones in module torch:\n",
      "\n",
      "ones(...)\n",
      "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`size`.\n",
      "    \n",
      "    Args:\n",
      "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "    \n",
      "    Keyword arguments:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理：生成csv文件，pandas读取cvs，并且利用pandas进行数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_file = os.path.join('.','house_tiny.csv')\n",
    "with open(data_file,\"w\") as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # 列名\n",
    "    f.write('NA,Pave,127500\\n')  # 每行表示一个数据样本\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumRooms    3.0\n",
      "dtype: float64\n",
      "==============================\n",
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n",
      "==============================\n",
      "0    127500\n",
      "1    106000\n",
      "2    178100\n",
      "3    140000\n",
      "Name: Price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 切片表示具体某一列时，是直接按索引号来的\n",
    "# 所以data.iloc[:, 3]取索引为3的列\n",
    "\n",
    "# cvs拆成两个部分，输入和输出\n",
    "inputs, outputs = data.iloc[:,0:2] ,data.iloc[:,2]\n",
    "# 是Nan的补上改该列均值\n",
    "inputs = inputs.fillna(inputs.mean(numeric_only=True))\n",
    "print(inputs.mean(numeric_only=True))\n",
    "print(\"==============================\")\n",
    "print(inputs)\n",
    "print(\"==============================\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fillna in module pandas.core.frame:\n",
      "\n",
      "fillna(value: 'Hashable | Mapping | Series | DataFrame' = None, *, method: 'FillnaOptions | None' = None, axis: 'Axis | None' = None, inplace: 'bool' = False, limit: 'int | None' = None, downcast: 'dict | None' = None) -> 'DataFrame | None' method of pandas.core.frame.DataFrame instance\n",
      "    Fill NA/NaN values using the specified method.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    value : scalar, dict, Series, or DataFrame\n",
      "        Value to use to fill holes (e.g. 0), alternately a\n",
      "        dict/Series/DataFrame of values specifying which value to use for\n",
      "        each index (for a Series) or column (for a DataFrame).  Values not\n",
      "        in the dict/Series/DataFrame will not be filled. This value cannot\n",
      "        be a list.\n",
      "    method : {'backfill', 'bfill', 'ffill', None}, default None\n",
      "        Method to use for filling holes in reindexed Series:\n",
      "    \n",
      "        * ffill: propagate last valid observation forward to next valid.\n",
      "        * backfill / bfill: use next valid observation to fill gap.\n",
      "    \n",
      "    axis : {0 or 'index', 1 or 'columns'}\n",
      "        Axis along which to fill missing values. For `Series`\n",
      "        this parameter is unused and defaults to 0.\n",
      "    inplace : bool, default False\n",
      "        If True, fill in-place. Note: this will modify any\n",
      "        other views on this object (e.g., a no-copy slice for a column in a\n",
      "        DataFrame).\n",
      "    limit : int, default None\n",
      "        If method is specified, this is the maximum number of consecutive\n",
      "        NaN values to forward/backward fill. In other words, if there is\n",
      "        a gap with more than this number of consecutive NaNs, it will only\n",
      "        be partially filled. If method is not specified, this is the\n",
      "        maximum number of entries along the entire axis where NaNs will be\n",
      "        filled. Must be greater than 0 if not None.\n",
      "    downcast : dict, default is None\n",
      "        A dict of item->dtype of what to downcast if possible,\n",
      "        or the string 'infer' which will try to downcast to an appropriate\n",
      "        equal type (e.g. float64 to int64 if possible).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        Object with missing values filled or None if ``inplace=True``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    interpolate : Fill NaN values using interpolation.\n",
      "    reindex : Conform object to new index.\n",
      "    asfreq : Convert TimeSeries to specified frequency.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
      "    ...                    [3, 4, np.nan, 1],\n",
      "    ...                    [np.nan, np.nan, np.nan, np.nan],\n",
      "    ...                    [np.nan, 3, np.nan, 4]],\n",
      "    ...                   columns=list(\"ABCD\"))\n",
      "    >>> df\n",
      "         A    B   C    D\n",
      "    0  NaN  2.0 NaN  0.0\n",
      "    1  3.0  4.0 NaN  1.0\n",
      "    2  NaN  NaN NaN  NaN\n",
      "    3  NaN  3.0 NaN  4.0\n",
      "    \n",
      "    Replace all NaN elements with 0s.\n",
      "    \n",
      "    >>> df.fillna(0)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  0.0  0.0\n",
      "    1  3.0  4.0  0.0  1.0\n",
      "    2  0.0  0.0  0.0  0.0\n",
      "    3  0.0  3.0  0.0  4.0\n",
      "    \n",
      "    We can also propagate non-null values forward or backward.\n",
      "    \n",
      "    >>> df.fillna(method=\"ffill\")\n",
      "         A    B   C    D\n",
      "    0  NaN  2.0 NaN  0.0\n",
      "    1  3.0  4.0 NaN  1.0\n",
      "    2  3.0  4.0 NaN  1.0\n",
      "    3  3.0  3.0 NaN  4.0\n",
      "    \n",
      "    Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n",
      "    2, and 3 respectively.\n",
      "    \n",
      "    >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
      "    >>> df.fillna(value=values)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  2.0  0.0\n",
      "    1  3.0  4.0  2.0  1.0\n",
      "    2  0.0  1.0  2.0  3.0\n",
      "    3  0.0  3.0  2.0  4.0\n",
      "    \n",
      "    Only replace the first NaN element.\n",
      "    \n",
      "    >>> df.fillna(value=values, limit=1)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  2.0  0.0\n",
      "    1  3.0  4.0  NaN  1.0\n",
      "    2  NaN  1.0  NaN  3.0\n",
      "    3  NaN  3.0  NaN  4.0\n",
      "    \n",
      "    When filling using a DataFrame, replacement happens along\n",
      "    the same column names and same indices\n",
      "    \n",
      "    >>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n",
      "    >>> df.fillna(df2)\n",
      "         A    B    C    D\n",
      "    0  0.0  2.0  0.0  0.0\n",
      "    1  3.0  4.0  0.0  1.0\n",
      "    2  0.0  0.0  0.0  NaN\n",
      "    3  0.0  3.0  0.0  4.0\n",
      "    \n",
      "    Note that column D is not affected since it is not present in df2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(inputs.fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0        True      False\n",
      "1       2.0       False       True\n",
      "2       4.0       False       True\n",
      "3       3.0       False       True\n"
     ]
    }
   ],
   "source": [
    "# numpy可以根据某一列的值是否是NaN,来拆分成两列\n",
    "# 即pd.get_dummies 可以将非数值的列，拆成多列，每一列代表一个值\n",
    "inputs = pd.get_dummies(inputs,dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数值类型转换为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 转成float\n",
    "inputs = inputs.astype(float)\n",
    "outputs = outputs.astype(float)\n",
    "\n",
    "# torch.tensor输入不能是对象，不知道为啥这里报错是对象， 转成float就可以了\n",
    "# 原ipynb没问题\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化一个矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.,  8., 12., 16.],\n",
       "        [ 1.,  5.,  9., 13., 17.],\n",
       "        [ 2.,  6., 10., 14., 18.],\n",
       "        [ 3.,  7., 11., 15., 19.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量：就是多个矩阵 、 矩阵的的更高维度的推广\n",
    "\n",
    "就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone()# 通过分配新内存，将A的一个副本分配给B\n",
    "A , A+B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " tensor([[[ 0,  2,  4,  6],\n",
       "          [ 8, 10, 12, 14],\n",
       "          [16, 18, 20, 22]],\n",
       " \n",
       "         [[24, 26, 28, 30],\n",
       "          [32, 34, 36, 38],\n",
       "          [40, 42, 44, 46]]]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, a * X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "x,x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]),\n",
       " tensor(190.),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape,A.sum(),A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=几，就是**去掉**这个轴，其它轴求和\n",
    "\n",
    "如下axis=0就是不按0轴求和，按1轴求和\n",
    "\n",
    "如果是多维，也可以指定多维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A.sum(axis=0)  axis=0按列求和，输出一个列表表示每一列的和\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A.sum(axis=1)  axis=1按行求和，输出一个列表表示每一行的和\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加上keepdims=True，则会保留去掉的这个维度，后面也常用\n",
    "\n",
    "可以看到加上后，还是三个[[[]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8., 8., 8., 8., 8.]),\n",
       " torch.Size([5]),\n",
       " tensor([[[8.],\n",
       "          [8.],\n",
       "          [8.],\n",
       "          [8.],\n",
       "          [8.]]]),\n",
       " torch.Size([1, 5, 1]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((2,5,4))\n",
    "\n",
    "a_sum_axis0 = a.sum(axis=[0,2])\n",
    "\n",
    "a_sum_axis0_T = a.sum(axis=[0,2],keepdims=True)\n",
    "\n",
    "a_sum_axis0 , a_sum_axis0.shape,a_sum_axis0_T , a_sum_axis0_T.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以沿指定轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量\n",
    "向量点积\n",
    "\n",
    "- 点乘： $a^Tb=\\sum_{i} a_ib_i$\n",
    "- 正交： $a^Tb=\\sum_{i} a_ib_i=0$ —> 垂直点积为0\n",
    "  - eg: a=[0,1] b=[1,0]\n",
    "  - $\\sum_{i} a_ib_i=0*1+1*0=0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵-向量积\n",
    "`torch.mv(A,x)`\n",
    "\n",
    "$\\text { 矩阵向量积 } \\mathbf{A x} \\text { 得到的是一个长度为 } m \\text { 的列向量, 其 } i^{\\mathrm{th}} \\text { 元素是点积 } \\mathbf{a}_{i}^{\\top} \\mathbf{x}$\n",
    "\n",
    "矩阵A的每个行向量和向量x点积，返回结果为每行的点积组成的列表/向量\n",
    "\n",
    "可以求解神经网络每一层所需的复杂计算\n",
    "\n",
    "也可以用方阵的乘法来表示旋转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " torch.Size([4]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "x = torch.arange(4,dtype=torch.float32)\n",
    "A,A.shape ,x,x.shape ,torch.mv(A,x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵-矩阵乘法\n",
    "\n",
    "`torch.mm(A,B)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B =torch.ones(4,3)\n",
    "torch.mm(A,B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵点积 Hadamard积⊙\n",
    "与矩阵乘法不同\n",
    "\n",
    "**两个矩阵的每个对应位置直接相乘**\n",
    "\n",
    "**要求矩阵必须维数相等**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[0, 1],\n",
       "         [4, 9]]))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.arange(4).reshape(2,2)\n",
    "A = B\n",
    "A,B,A*B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 范数\n",
    "\n",
    "线性代数中最有用的一些运算符是*范数*（norm）。\n",
    "非正式地说，向量的*范数*是表示一个向量有多大。\n",
    "这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。\n",
    "\n",
    "在线性代数中，向量范数是将向量映射到标量的函数$f$。\n",
    "给定任意向量$\\mathbf{x}$，向量范数要满足一些属性。\n",
    "第一个性质是：如果我们按常数因子$\\alpha$缩放向量的所有元素，\n",
    "其范数也会按相同常数因子的*绝对值*缩放：\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "第二个性质是熟悉的三角不等式:\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "第三个性质简单地说范数必须是非负的:\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。\n",
    "最后一个性质要求范数最小为0，当且仅当向量全由0组成。\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "范数听起来很像距离的度量。\n",
    "欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。\n",
    "事实上，欧几里得距离是一个$L_2$范数：\n",
    "假设$n$维向量$\\mathbf{x}$中的元素是$x_1,\\ldots,x_n$，其[**$L_2$*范数*是向量元素平方和的平方根：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n",
    "\n",
    "其中，在$L_2$范数中常常省略下标$2$，也就是说$\\|\\mathbf{x}\\|$等同于$\\|\\mathbf{x}\\|_2$。\n",
    "在代码中，我们可以按如下方式计算向量的$L_2$范数。\n",
    "\n",
    "`torch.norm(u)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0,-4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1范数\n",
    "\n",
    "`torch.abs(u).sum()`\n",
    "\n",
    "深度学习中更经常地使用$L_2$范数的平方，也会经常遇到[**$L_1$范数，它表示为向量元素的绝对值之和：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
    "\n",
    "与$L_2$范数相比，$L_1$范数受异常值的影响较小。\n",
    "为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性代数的其他应用\n",
    "\n",
    "矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构\n",
    "\n",
    "机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵的范数\n",
    "矩阵的 *费罗贝尼乌斯范数(Frobenius norm)* 是矩阵元素的平方的平方根\n",
    "类似于向量的$L_2$范数\n",
    "\n",
    "(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n",
    "\n",
    "Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的$L_2$范数。\n",
    "\n",
    "调用以下函数将计算矩阵的Frobenius范数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用以下函数将计算矩阵的Frobenius范数。\n",
    "\n",
    "torch.norm(torch.ones((4,9)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 导数和微分\n",
    " \n",
    " 在深度学习中，我们通常选择对于模型参数<u>**可微**</u>的损失函数。\n",
    " \n",
    " 讲人话就是，对于每个参数，如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多块的<u>**速度**</u>增加或减少 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline 是干什么的？\n",
    "# pycharm不能用，jupyter可以用\n",
    "# 用了就不用plt.show()了，图像plot之后，会自动显示\n",
    "# inline参数表示在当前页面显示\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# backend_inline函数用来导入 Matplotlib 的 Inline 后端,就可以在当前单元格中直接绘制图表了\n",
    "# 默认情况下，Matplotlib 并不会嵌入到 Jupyter Notebook 中，而是在后台运行\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个数学公式\n",
    "(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**)\n",
    "\n",
    "(**定义$u=f(x)=3x^2-4x$**)如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000,numerical limit=2.30000\n",
      "h=0.01000,numerical limit=2.03000\n",
      "h=0.00100,numerical limit=2.00300\n",
      "h=0.00010,numerical limit=2.00030\n",
      "h=0.00001,numerical limit=2.00003\n",
      "h=0.00000,numerical limit=2.00000\n",
      "h=0.00000,numerical limit=2.00000\n",
      "h=0.00000,numerical limit=2.00000\n",
      "h=0.00000,numerical limit=2.00000\n",
      "h=0.00000,numerical limit=2.00000\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "def f(x):\n",
    "    return 3*x**2-4*x\n",
    "\n",
    "def numerical_lim(f,x,h):\n",
    "    return (f(x+h)-f(x))/h\n",
    "\n",
    "h=0.1\n",
    "for i in range(10):\n",
    "    print(f'h={h:.5f},numerical limit={numerical_lim(f,1,h):.5f}')\n",
    "    h*=0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面出现小数精度不够问题，五位小数之后就不准了\n",
    "\n",
    "因为python采用底层的浮点数运算，不可避免\n",
    "\n",
    "所以用decimal包解决\n",
    "\n",
    "但也没有解决上面的问题，暂留。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "a = Decimal('4.2')\n",
    "b = Decimal('2.1')\n",
    "print(a+b)\n",
    "# 使用与否对比\n",
    "a+b==Decimal('6.3'), a+b==6.3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**为了对导数的这种解释进行可视化，我们将使用`matplotlib`**]，\n",
    "这是一个Python中流行的绘图库。\n",
    "要配置`matplotlib`生成图形的属性，我们需要(**定义几个函数**)。\n",
    "在下面，`use_svg_display`函数指定`matplotlib`软件包输出svg图表以获得更清晰的图像。\n",
    "\n",
    "注意，注释`#@save`是一个特殊的标记，表示这个函数在d2l中有过实现。\n",
    "\n",
    "后面可以直接调用，而不是老师书里说的可以通过这个标记添加到d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend_inline在jupyter中显示图像\n",
    "# set_matplotlib_formats函数用来设置图像格式\n",
    "def use_svg_display(): #@save\n",
    "    \"\"\"使用svg格式在jupyter中显示绘图\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams是一个全局字典变量，用来存储matplotlib的配置信息\n",
    "# 'figure.figsize'是图像的尺寸\n",
    "def set_figsize(figsize=(3.5,2.5)): #@save\n",
    "    \"\"\"设置matplotlib的图表大小\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "\n",
    "def set_axes(axes,xlabel,ylabel,xlim,ylim,xscale,yscale,\n",
    "             legend): # 图例就是表示图中的每一条线的含义的东西\n",
    "    \"\"\"设置matplotlib的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)# axes.set_xscale设置x轴的比例\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim) # axes.set_xlim设置x轴的范围\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend: # legend是一个列表，表示图例\n",
    "        axes.legend(legend)  # axes.legend设置图例\n",
    "    axes.grid() # grid是网格，显示网格\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面这三个用于图形配置的函数，定义一个plot函数来简洁地绘制多条曲线， 因为我们需要在整个书中可视化许多曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "\n",
    "def plot(X,Y=None,xlabel=None,ylabel=None,legend=None,xlim=None,\n",
    "         ylim=None,xscale='linear',yscale='linear',\n",
    "         fmts=('-','m--','g-.','r:'), # fmts是一个元组，表示线的格式\n",
    "         figsize=(3.5,2.5),axes=None):\n",
    "    \"\"\"绘制数据点\"\"\"\n",
    "    if legend is None: \n",
    "        legend = [] # 如果没有图例，就把图例设置成空列表\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca() # gca是get current axes的缩写，获取当前的轴\n",
    "\n",
    "    # 如果X有一个轴，直接返回，输出True\n",
    "    def has_one_axis(X):\n",
    "        # 用了如下检测X只有一个轴的方法\n",
    "        # isinstance(X,list) and not hasattr(X[0],\"__len__\") 表示X是一个列表，且X[0]没有__len__属性\n",
    "        # hasattr(X,\"ndim\") and X.ndim==1 表示X有ndim属性，且ndim=1\n",
    "        return (hasattr(X,\"ndim\") and X.ndim==1 or isinstance(X,list)\n",
    "                and not hasattr(X[0],\"__len__\"))\n",
    "    \n",
    "    # 如果X只有一个轴，就把X变成列表，Y也变成列表\n",
    "    if has_one_axis(X):\n",
    "        # [X]表示把X变成列表\n",
    "        # 如果X不是列表，就把X变成列表，如果X是列表也不影响\n",
    "        # 保证执行完之后，X一定是列表\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X,Y = [[]] * len(X),X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    # zip函数用来打包X和Y，zip(X,Y)表示把X和Y打包成一个元组,比如\n",
    "    # X = [[1,2,3],[4,5,6]]\n",
    "    if len(X) !=len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla() # cla是clear axes的缩写，清除轴\n",
    "    for x,y,fmt in zip(X,Y,fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x,y,fmt) # axes.plot绘制图像\n",
    "        else:\n",
    "            axes.plot(y,fmt)\n",
    "    # set_axes函数用来设置轴\n",
    "    set_axes(axes,xlabel,ylabel,xlim,ylim,xscale,yscale,legend)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以[**绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$**]，\n",
    "其中系数$2$是切线的斜率。\n",
    "\n",
    "切线方程是根据x=1带入f(x)算出斜率和bias求出来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.807484pt\" height=\"183.634375pt\" viewBox=\"0 0 243.807484 183.634375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-05-27T11:57:56.823300</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 183.634375 \n",
       "L 243.807484 183.634375 \n",
       "L 243.807484 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.88125 145.8 \n",
       "L 236.18125 145.8 \n",
       "L 236.18125 7.2 \n",
       "L 40.88125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 49.758523 145.8 \n",
       "L 49.758523 7.2 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m3ef53da518\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3ef53da518\" x=\"49.758523\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(46.577273 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 110.981093 145.8 \n",
       "L 110.981093 7.2 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3ef53da518\" x=\"110.981093\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(107.799843 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 172.203664 145.8 \n",
       "L 172.203664 7.2 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3ef53da518\" x=\"172.203664\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(169.022414 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 233.426234 145.8 \n",
       "L 233.426234 7.2 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m3ef53da518\" x=\"233.426234\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(230.244984 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- linsX_x -->\n",
       "     <g transform=\"translate(121.095313 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-58\" d=\"M 403 4666 \n",
       "L 1081 4666 \n",
       "L 2241 2931 \n",
       "L 3406 4666 \n",
       "L 4084 4666 \n",
       "L 2584 2425 \n",
       "L 4184 0 \n",
       "L 3506 0 \n",
       "L 2194 1984 \n",
       "L 872 0 \n",
       "L 191 0 \n",
       "L 1856 2491 \n",
       "L 403 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"55.566406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"118.945312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-58\" x=\"171.044922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"239.550781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"289.550781\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 40.88125 116.769994 \n",
       "L 236.18125 116.769994 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"mc4619f4112\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc4619f4112\" x=\"40.88125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.51875 120.569213) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 40.88125 78.886651 \n",
       "L 236.18125 78.886651 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc4619f4112\" x=\"40.88125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(27.51875 82.685869) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 40.88125 41.003307 \n",
       "L 236.18125 41.003307 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc4619f4112\" x=\"40.88125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(21.15625 44.802526) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- linsY_f(x) -->\n",
       "     <g transform=\"translate(14.798438 99.226563) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-59\" d=\"M -13 4666 \n",
       "L 666 4666 \n",
       "L 1959 2747 \n",
       "L 3244 4666 \n",
       "L 3922 4666 \n",
       "L 2272 2222 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 2222 \n",
       "L -13 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"55.566406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"118.945312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-59\" x=\"171.044922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.128906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-66\" x=\"282.128906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"317.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"356.347656\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"415.527344\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 49.758523 116.769994 \n",
       "L 55.88078 119.573361 \n",
       "L 62.003037 121.922129 \n",
       "L 68.125294 123.816296 \n",
       "L 74.247551 125.255863 \n",
       "L 80.369808 126.24083 \n",
       "L 86.492065 126.771197 \n",
       "L 92.614322 126.846963 \n",
       "L 98.736579 126.46813 \n",
       "L 104.858836 125.634696 \n",
       "L 110.981093 124.346663 \n",
       "L 117.10335 122.604029 \n",
       "L 123.225607 120.406795 \n",
       "L 129.347864 117.754961 \n",
       "L 135.470121 114.648527 \n",
       "L 141.592379 111.087492 \n",
       "L 147.714636 107.071858 \n",
       "L 153.836893 102.601624 \n",
       "L 159.95915 97.676789 \n",
       "L 166.081407 92.297354 \n",
       "L 172.203664 86.463319 \n",
       "L 178.325921 80.174684 \n",
       "L 184.448178 73.431449 \n",
       "L 190.570435 66.233614 \n",
       "L 196.692692 58.581179 \n",
       "L 202.814949 50.474143 \n",
       "L 208.937206 41.912508 \n",
       "L 215.059463 32.896272 \n",
       "L 221.18172 23.425436 \n",
       "L 227.303977 13.5 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 49.758523 139.5 \n",
       "L 55.88078 137.984666 \n",
       "L 62.003037 136.469333 \n",
       "L 68.125294 134.953999 \n",
       "L 74.247551 133.438665 \n",
       "L 80.369808 131.923331 \n",
       "L 86.492065 130.407998 \n",
       "L 92.614322 128.892664 \n",
       "L 98.736579 127.37733 \n",
       "L 104.858836 125.861996 \n",
       "L 110.981093 124.346663 \n",
       "L 117.10335 122.831329 \n",
       "L 123.225607 121.315995 \n",
       "L 129.347864 119.800661 \n",
       "L 135.470121 118.285328 \n",
       "L 141.592379 116.769994 \n",
       "L 147.714636 115.25466 \n",
       "L 153.836893 113.739327 \n",
       "L 159.95915 112.223993 \n",
       "L 166.081407 110.708659 \n",
       "L 172.203664 109.193325 \n",
       "L 178.325921 107.677992 \n",
       "L 184.448178 106.162658 \n",
       "L 190.570435 104.647324 \n",
       "L 196.692692 103.13199 \n",
       "L 202.814949 101.616657 \n",
       "L 208.937206 100.101323 \n",
       "L 215.059463 98.585989 \n",
       "L 221.18172 97.070655 \n",
       "L 227.303977 95.555322 \n",
       "\" clip-path=\"url(#p8c39fed023)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.88125 145.8 \n",
       "L 40.88125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 236.18125 145.8 \n",
       "L 236.18125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.88125 145.8 \n",
       "L 236.18125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.88125 7.2 \n",
       "L 236.18125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 47.88125 44.55625 \n",
       "L 169.253125 44.55625 \n",
       "Q 171.253125 44.55625 171.253125 42.55625 \n",
       "L 171.253125 14.2 \n",
       "Q 171.253125 12.2 169.253125 12.2 \n",
       "L 47.88125 12.2 \n",
       "Q 45.88125 12.2 45.88125 14.2 \n",
       "L 45.88125 42.55625 \n",
       "Q 45.88125 44.55625 47.88125 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 49.88125 20.298438 \n",
       "L 59.88125 20.298438 \n",
       "L 69.88125 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(77.88125 23.798438) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 49.88125 34.976562 \n",
       "L 59.88125 34.976562 \n",
       "L 69.88125 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Tangent line(x=1) -->\n",
       "     <g transform=\"translate(77.88125 38.476562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \n",
       "L 4684 2906 \n",
       "L 4684 2381 \n",
       "L 678 2381 \n",
       "L 678 2906 \n",
       "z\n",
       "M 678 1631 \n",
       "L 4684 1631 \n",
       "L 4684 1100 \n",
       "L 678 1100 \n",
       "L 678 1631 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"609.085938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"648.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"707.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" x=\"791.068359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"854.691406\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p8c39fed023\">\n",
       "   <rect x=\"40.88125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x =np.arange(0,3,0.1)\n",
    "plot(x,[f(x),2*x-3],'linsX_x','linsY_f(x)',legend=['f(x)','Tangent line(x=1)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 亚导数\n",
    "- 就是不可导的地方，找个东西代替\n",
    "\n",
    "- 比如y=|x|在x=0处不可导，在x=0导数就让他等于a,然后a可以取[-1,1]之间任意的数\n",
    "- $$\\frac{\\partial|x|}{\\partial x}=\\left\\{\\begin{array}{ll}\n",
    "    1 & \\text { if } x>0 \\\\\n",
    "    -1 & \\text { if } x<0 \\\\\n",
    "    a & \\text { if } x=0, \\quad a \\in[-1,1]\n",
    "    \\end{array}\\right.$$\n",
    "\n",
    "- 再比如y=max(x,0)，在x=0处，也等于a，a可以取[0,1]之间任意数\n",
    "- $$\\frac{\\partial}{\\partial x}=\\left\\{\\begin{array}{ll}\n",
    "    1 & \\text { if } x>0 \\\\\n",
    "    0 & \\text { if } x<0 \\\\\n",
    "    a & \\text { if } x=0, \\quad a \\in[0,1]\n",
    "    \\end{array}\\right.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度(导数扩展到向量)\n",
    "- 所有一阶偏导数组成的一个向量\n",
    "- 我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度<u>**向量**</u>\n",
    "\n",
    "- 具体而言，设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n",
    "一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$，并且输出是一个标量。\n",
    "- 函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n",
    "\n",
    "- $$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
    "\n",
    "- 其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 雅可比矩阵(Jacobian matrix)\n",
    "- 所有一阶偏导数组成的矩阵\n",
    "\n",
    "- 跟梯度看起来一样，不过他是矩阵，梯度是向量\n",
    "\n",
    "\n",
    "- $y=f(x)$ y与x都是向量时，雅可比矩阵(Jacobian matrix)为\n",
    "\n",
    "- $J=\\left(\\begin{array}{ccc}\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\\end{array}\\right)$\n",
    "\n",
    "- $y=f(x)$ y为标量与x是向量时，雅可比矩阵(Jacobian matrix)为一维的\n",
    "\n",
    "- $J=\\left(\\begin{array}{lll}\\frac{\\partial y}{\\partial x_{1}} & \\cdots & \\frac{\\partial y}{\\partial x_{m}}\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "\n",
    "求函数$f(\\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即分别每个自变量的偏导数，组成一个向量即可\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x_{1}}=6 x_{1}$ , $\\frac{\\delta f}{\\delta x_{2}}=5e^{x_2}$\n",
    "\n",
    "则$\\frac{\\delta f}{\\delta x}=\\left(6 x_{1}, 5 e^{x_{2}}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵计算\n",
    "\n",
    "李沐讲的太短了，听不懂的。这里也就看了下第一个，深度学习只对标量函数求导，就没看后面的了。\n",
    "\n",
    "知道一些矩阵求导的常见等式感觉暂时能看懂就行了\n",
    "\n",
    "矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇）：https://zhuanlan.zhihu.com/p/263777564\n",
    "\n",
    "矩阵求导公式的数学推导（矩阵求导——基础篇）：https://zhuanlan.zhihu.com/p/273729929\n",
    "\n",
    "矩阵求导公式的数学推导（矩阵求导——进阶篇）：https://zhuanlan.zhihu.com/p/288541909\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "- PyTorch提供的$autograd$包能够根据输⼊和前向传播过程⾃动构建**计算图**，并执⾏反向传播。\n",
    "\n",
    "- (**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。\n",
    "\n",
    "- 并且声明backward()时需要对x求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True), None)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0,requires_grad=True)\n",
    "x,x.grad # 默认值是None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2*torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y.backward()进行反向传播,即计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正常函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下pytorch会累积梯度，所以再次计算梯度时需要清楚之前的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(6., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # 梯度清零\n",
    "# 换一个y的函数看看\n",
    "y = x.sum() # 求和 ，y是一个标量了\n",
    "# 但计算图还在，对x求导还是可以的\n",
    "y.backward()\n",
    "x.grad,x.sum() # 结果会与下面的不一样，因为y的函数表达式不一样"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非标量变量的反向传播\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "但当y是向量或矩阵时，再加上网络多层的叠加，计算量会爆炸。\n",
    "\n",
    "当然理论上也是可以的，但需要自己找办法实现\n",
    "pytorch也没有支持，当y不是标量时会报错。\n",
    "\n",
    "通过y.sum()将y转换为标量。然后再对x求导。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "# 通过y.sum()将y转换为标量。\n",
    "# 然后再对x求导。 x²的导数是2x ，因为计算图还在\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将某些计算移动到记录的计算图之外\n",
    "\n",
    "`u=y.detach()`\n",
    "\n",
    "某些情况下会需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# u不是x的函数，所以z对x求导的时候，u看作常数\n",
    "u = y.detach() # u是y的副本，但是计算图已经被丢弃了\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 即使构建函数的计算图需要通过python控制流，我们仍然可以计算得到变量的梯度\n",
    "\n",
    "Python控制流（例如，条件、循环或任意函数调用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1024., 1024., 1024.])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.randn(3,requires_grad=True)\n",
    "d = f(a) # 函数最终为 f(a)=k*a ,k是某个常量，所以梯度（导数）为d/a\n",
    "d.sum().backward()\n",
    "#d.backward() #报错，Trying to backward through the graph a second time\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad==d/a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习\n",
    "\n",
    "2.在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "\n",
    "报错：Trying to backward through the graph a second time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.把a变成矩阵/向量，会发生什么？\n",
    "\n",
    "也会报错，不能直接反向传播\n",
    "\n",
    "要`d.sum().backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3301, requires_grad=True)\n",
      "tensor(-0.4756, requires_grad=True)\n",
      "tensor([[ 0.0808,  1.3532,  1.3845,  0.5473],\n",
      "        [ 0.4318, -0.5792,  0.0093,  1.4463],\n",
      "        [ 0.2392,  0.3775, -0.1730, -0.6949]])\n",
      "tensor([1.0933, 2.2052, 1.5617], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 随机生成一个a\n",
    "# torch.randn就是生成随机数的函数，size是其第一个参数\n",
    "# 这里指定size=() ,即指定size为空元组，表示生成一个标量\n",
    "a =torch.randn(size=(),requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "# 不加size也可以\n",
    "a =torch.randn((),requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "# 生成一个形状为 (3, 4) 的随机张量\n",
    "a = torch.randn(size=(3, 4))\n",
    "print(a)\n",
    "# 生成列表\n",
    "a =torch.randn(3,requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n",
    "\n",
    "**梯度都是存在`x.grad`里面的,调用即可得到梯度值（偏导数值）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x252f4378a60>]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"240.982812pt\" height=\"169.678125pt\" viewBox=\"0 0 240.982812 169.678125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-05-27T21:09:52.565550</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 169.678125 \n",
       "L 240.982813 169.678125 \n",
       "L 240.982813 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 233.782813 145.8 \n",
       "L 233.782813 7.2 \n",
       "L 38.482813 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m00d05a4987\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00d05a4987\" x=\"47.360085\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(44.178835 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00d05a4987\" x=\"83.227855\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(80.046605 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00d05a4987\" x=\"119.095625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(115.914375 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00d05a4987\" x=\"154.963395\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(151.782145 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00d05a4987\" x=\"190.831165\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(187.649915 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m00d05a4987\" x=\"226.698935\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(220.336435 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m39b146f46a\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m39b146f46a\" x=\"38.482813\" y=\"139.504837\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(7.2 143.304055) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m39b146f46a\" x=\"38.482813\" y=\"108.003628\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(7.2 111.802846) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m39b146f46a\" x=\"38.482813\" y=\"76.502418\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(15.579688 80.301637) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m39b146f46a\" x=\"38.482813\" y=\"45.001209\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(15.579688 48.800428) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m39b146f46a\" x=\"38.482813\" y=\"13.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(15.579688 17.299219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 47.360085 76.502418 \n",
       "L 49.153474 70.212671 \n",
       "L 50.946862 63.98577 \n",
       "L 52.740251 57.88393 \n",
       "L 54.533639 51.96812 \n",
       "L 56.327028 46.297449 \n",
       "L 58.120417 40.928576 \n",
       "L 59.913805 35.915147 \n",
       "L 61.707193 31.30725 \n",
       "L 63.500582 27.150928 \n",
       "L 65.29397 23.487713 \n",
       "L 67.087359 20.354197 \n",
       "L 68.880748 17.781702 \n",
       "L 70.674135 15.795922 \n",
       "L 72.467524 14.416702 \n",
       "L 74.260913 13.657821 \n",
       "L 76.054302 13.526865 \n",
       "L 77.847691 14.025136 \n",
       "L 79.641079 15.147663 \n",
       "L 81.434466 16.883224 \n",
       "L 83.227855 19.214483 \n",
       "L 85.021242 22.118137 \n",
       "L 86.814633 25.565193 \n",
       "L 88.60802 29.521184 \n",
       "L 90.401411 33.946611 \n",
       "L 92.194798 38.797228 \n",
       "L 93.988189 44.024592 \n",
       "L 95.781576 49.576454 \n",
       "L 97.574967 55.397366 \n",
       "L 99.368353 61.429137 \n",
       "L 101.16174 67.611517 \n",
       "L 102.955131 73.882745 \n",
       "L 104.748518 80.180134 \n",
       "L 106.541905 86.440775 \n",
       "L 108.335296 92.602132 \n",
       "L 110.128683 98.60261 \n",
       "L 111.922074 104.382286 \n",
       "L 113.715461 109.883379 \n",
       "L 115.508847 115.050941 \n",
       "L 117.302238 119.833356 \n",
       "L 119.095625 124.182806 \n",
       "L 120.889012 128.055852 \n",
       "L 122.682399 131.413792 \n",
       "L 124.475794 134.223092 \n",
       "L 126.269181 136.455652 \n",
       "L 128.062568 138.08918 \n",
       "L 129.855954 139.107352 \n",
       "L 131.649341 139.5 \n",
       "L 133.442737 139.263195 \n",
       "L 135.236123 138.399306 \n",
       "L 137.02951 136.916968 \n",
       "L 138.822906 134.830975 \n",
       "L 140.616292 132.162188 \n",
       "L 142.409679 128.937274 \n",
       "L 144.203066 125.188447 \n",
       "L 145.996453 120.953163 \n",
       "L 147.789839 116.273748 \n",
       "L 149.583226 111.196949 \n",
       "L 151.376613 105.773496 \n",
       "L 153.170008 100.057547 \n",
       "L 154.963395 94.10627 \n",
       "L 156.756782 87.979103 \n",
       "L 158.550169 81.737263 \n",
       "L 160.343556 75.44312 \n",
       "L 162.136951 69.159531 \n",
       "L 163.930338 62.949339 \n",
       "L 165.723724 56.874564 \n",
       "L 167.51712 50.995878 \n",
       "L 169.310507 45.372073 \n",
       "L 171.103893 40.05931 \n",
       "L 172.89728 35.110674 \n",
       "L 174.690667 30.57561 \n",
       "L 176.484054 26.499432 \n",
       "L 178.277441 22.922865 \n",
       "L 180.070827 19.881646 \n",
       "L 181.864223 17.406152 \n",
       "L 183.657609 15.52114 \n",
       "L 185.450996 14.245433 \n",
       "L 187.244383 13.591774 \n",
       "L 189.03777 13.566697 \n",
       "L 190.831165 14.170456 \n",
       "L 192.62456 15.397022 \n",
       "L 194.417939 17.234114 \n",
       "L 196.211334 19.663418 \n",
       "L 198.004712 22.660607 \n",
       "L 199.798108 26.195798 \n",
       "L 201.591503 30.233643 \n",
       "L 203.384881 34.733746 \n",
       "L 205.178277 39.651229 \n",
       "L 206.971672 44.936923 \n",
       "L 208.76505 50.537957 \n",
       "L 210.558445 56.398473 \n",
       "L 212.351824 62.459803 \n",
       "L 214.145219 68.6615 \n",
       "L 215.938614 74.941543 \n",
       "L 217.731993 81.237121 \n",
       "L 219.525388 87.48545 \n",
       "L 221.318766 93.623983 \n",
       "L 223.112162 99.591501 \n",
       "L 224.90554 105.328265 \n",
       "\" clip-path=\"url(#p49d420c064)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path d=\"M 47.360085 13.5 \n",
       "L 49.153474 13.814749 \n",
       "L 50.946862 14.755853 \n",
       "L 52.740251 16.313908 \n",
       "L 54.533639 18.473349 \n",
       "L 56.327028 21.212595 \n",
       "L 58.120417 24.504278 \n",
       "L 59.913805 28.315509 \n",
       "L 61.707193 32.608211 \n",
       "L 63.500582 37.33949 \n",
       "L 65.29397 42.462065 \n",
       "L 67.087359 47.924766 \n",
       "L 68.880748 53.673007 \n",
       "L 70.674135 59.649342 \n",
       "L 72.467524 65.794076 \n",
       "L 74.260913 72.045804 \n",
       "L 76.054302 78.34206 \n",
       "L 77.847691 84.619936 \n",
       "L 79.641079 90.816704 \n",
       "L 81.434466 96.870441 \n",
       "L 83.227855 102.720676 \n",
       "L 85.021242 108.30894 \n",
       "L 86.814633 113.579414 \n",
       "L 88.60802 118.479416 \n",
       "L 90.401411 122.960011 \n",
       "L 92.194798 126.976402 \n",
       "L 93.988189 130.488487 \n",
       "L 95.781576 133.461151 \n",
       "L 97.574967 135.864709 \n",
       "L 99.368353 137.675131 \n",
       "L 101.16174 138.87434 \n",
       "L 102.955131 139.450348 \n",
       "L 104.748518 139.397403 \n",
       "L 106.541905 138.716034 \n",
       "L 108.335296 137.413042 \n",
       "L 110.128683 135.501454 \n",
       "L 111.922074 133.000365 \n",
       "L 113.715461 129.93477 \n",
       "L 115.508847 126.3353 \n",
       "L 117.302238 122.237905 \n",
       "L 119.095625 117.683546 \n",
       "L 120.889012 112.717723 \n",
       "L 122.682399 107.390046 \n",
       "L 124.475794 101.753725 \n",
       "L 126.269181 95.865127 \n",
       "L 128.062568 89.783064 \n",
       "L 129.855954 83.568305 \n",
       "L 131.649341 77.282946 \n",
       "L 133.442737 70.989759 \n",
       "L 135.236123 64.751682 \n",
       "L 137.02951 58.631014 \n",
       "L 138.822906 52.688885 \n",
       "L 140.616292 46.98472 \n",
       "L 142.409679 41.575483 \n",
       "L 144.203066 36.515227 \n",
       "L 145.996453 31.854508 \n",
       "L 147.789839 27.639897 \n",
       "L 149.583226 23.913501 \n",
       "L 151.376613 20.712556 \n",
       "L 153.170008 18.06903 \n",
       "L 154.963395 16.009369 \n",
       "L 156.756782 14.554129 \n",
       "L 158.550169 13.717856 \n",
       "L 160.343556 13.508907 \n",
       "L 162.136951 13.929366 \n",
       "L 163.930338 14.975034 \n",
       "L 165.723724 16.635465 \n",
       "L 167.51712 18.894075 \n",
       "L 169.310507 21.72828 \n",
       "L 171.103893 25.109769 \n",
       "L 172.89728 29.004753 \n",
       "L 174.690667 33.37432 \n",
       "L 176.484054 38.174805 \n",
       "L 178.277441 43.358248 \n",
       "L 180.070827 48.872855 \n",
       "L 181.864223 54.663555 \n",
       "L 183.657609 60.672436 \n",
       "L 185.450996 66.839483 \n",
       "L 187.244383 73.103078 \n",
       "L 189.03777 79.400639 \n",
       "L 190.831165 85.669272 \n",
       "L 192.62456 91.846312 \n",
       "L 194.417939 97.869984 \n",
       "L 196.211334 103.680214 \n",
       "L 198.004712 109.218837 \n",
       "L 199.798108 114.430625 \n",
       "L 201.591503 119.263442 \n",
       "L 203.384881 123.668958 \n",
       "L 205.178277 127.603245 \n",
       "L 206.971672 131.026947 \n",
       "L 208.76505 133.905828 \n",
       "L 210.558445 136.211179 \n",
       "L 212.351824 137.91992 \n",
       "L 214.145219 139.015015 \n",
       "L 215.938614 139.485497 \n",
       "L 217.731993 139.326677 \n",
       "L 219.525388 138.540131 \n",
       "L 221.318766 137.133735 \n",
       "L 223.112162 135.121518 \n",
       "L 224.90554 132.523623 \n",
       "\" clip-path=\"url(#p49d420c064)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 38.482813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 233.782813 145.8 \n",
       "L 233.782813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 233.782813 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 38.482813 7.2 \n",
       "L 233.782813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p49d420c064\">\n",
       "   <rect x=\"38.482813\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Only Tensors of floating point and complex dtype can require gradients\n",
    "# 只有浮点数和复数类型的张量才能需要梯度\n",
    "x=torch.arange(0.0,10.0,0.1,requires_grad=True,dtype=torch.float32)\n",
    "\n",
    "# 在绘制图像时，Matplotlib 会尝试将 tensor 转换为 NumPy 数组\n",
    "# 正常的tensor没问题，但是带梯度的tensor会报错\n",
    "# 这里x，y都带\n",
    " \n",
    "# 去掉梯度，用来画图\n",
    "x1 = x.detach()\n",
    "y1=torch.sin(x1)\n",
    "# 用来求梯度的不能传入plt.plot\n",
    "y2=torch.sin(x)\n",
    "y2.sum().backward()\n",
    "plt.plot(x1,y1)\n",
    "# 由于x.grad张量需要梯度，因此会出现错误。\n",
    "plt.plot(x1,x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率\n",
    "- **概率给了我们一种正式的途径来说明我们的确定性水平。**\n",
    "\n",
    "  - 如果我们完全肯定图像是一只猫，我们说标签$y$是\"猫\"的*概率*，表示为$P(y=$\"猫\"$)$等于$1$。\n",
    "\n",
    "  - 如果我们不十分确定图像描绘的是一只猫，我们可以将概率赋值为$0.5<P(y=$\"猫\"$)<1$。\n",
    "\n",
    "###  大数定律（law of large numbers）： \n",
    "- def:  指在随机试验中，每次出现的结果不同，但是大量重复试验出现的结果的平均值却几乎总是接近于某个确定的值。\n",
    "\n",
    "- 样本数量越多，则其算术平均值就有越高的概率接近期望值。\n",
    "\n",
    "### Sampling(抽样) && 分布(distribution)\n",
    "- 在统计学中，我们把从概率分布中抽取样本的过程称为抽样（sampling）。\n",
    "\n",
    " - 笼统来说，可以把分布（distribution）看作对事件的概率分配， 稍后我们将给出的更正式定义。 \n",
    "\n",
    "- 将概率分配给一些**离散**选择的分布称为多项分布（multinomial distribution）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 联合概率（joint probability）\n",
    "- 就是A=a,B=a事件同时发生的概率,那必然小于等于某个事件单独发生的概率\n",
    "\n",
    "- 表示为$P(A=a,B=b)$  ；   $P(A=a,B=b)≤P(A=a)$\n",
    "\n",
    "- 也可以写成P(A,B)或者P(AB)，是一样的\n",
    "\n",
    "#### 条件概率\n",
    "- 分子就是AB的联合概率，即AB同时发生的概率\n",
    "\n",
    "- 条件概率比联合概率更小：在B发生的提前下，发生A，空间已经缩小了\n",
    "  - 联合概率的不等式带给我们一个有趣的比率：\n",
    "$0 \\leq \\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$。\n",
    "\n",
    "- $P(B=b \\mid A=a)= \\frac{P(A=a, B=b)}{P(A=a)}= \\frac{P(A,B)}{P(A)}$\n",
    "#### 乘法法则\n",
    "- 由*乘法法则*（multiplication rule ）可得到$P(A, B) = P(B \\mid A) P(A)$。\n",
    "\n",
    "#### 独立\n",
    "若变量独立的，才有$P(A, B) = P(A)P(B) $\n",
    "\n",
    "#### 贝叶斯定理(Bayes's theorem)\n",
    "\n",
    "- 由对称性，可得到$P(A, B) = P(A \\mid B) P(B)$。\n",
    "\n",
    "- 假设$P(B)>0$，求解其中一个条件变量，我们得到\n",
    "\n",
    "\n",
    "- $$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.$$\n",
    "\n",
    "- 请注意，这里我们使用紧凑的表示法：\n",
    "其中$P(A, B)$是一个*联合分布*（joint distribution），"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 边际化\n",
    "就是说可以通过下面的式子求出B的概率，就叫边际概率，其实很常用\n",
    "\n",
    "理解为：A、B两个事件，列出所有A的情况，在所有A情况下B发生的概率，求和就是B的概率\n",
    "\n",
    "为了能进行事件概率求和，我们需要*求和法则*（sum rule），\n",
    "\n",
    "即$B$的概率相当于计算$A$的所有可能选择，并将所有选择的联合概率聚合在一起：\n",
    "\n",
    "$$P(B) = \\sum_{A} P(A, B),$$\n",
    "\n",
    "这也称为*边际化*（marginalization）。\n",
    "边际化结果的概率或分布称为*边际概率*（marginal probability）\n",
    "或*边际分布*（marginal distribution）。\n",
    "\n",
    "维基百科：在实际应用中，例如人工神经网络的神经元互相关联，在计算它们各自的参数的时候，就会使用边缘分布计算得到某一特定神经元（变量）的值。\n",
    "\n",
    "应用没太明白。暂留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_pytorch",
   "language": "python",
   "name": "d2l_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
