{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二维卷积（互相关运算） `corr2d()`\n",
    "- 卷积之后，输出大小等于输入大小$n_h \\times n_w$减去卷积核大小$k_h \\times k_w$，即：\n",
    "  - $$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
    "\n",
    "- 我们在`corr2d`函数中实现二维卷积运算\n",
    "  - 该函数接受输入张量`X`和卷积核张量`K`，并返回输出张量`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X,K):\n",
    "    \"\"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # h ,w 保存卷积核大小\n",
    "    Y = torch.zeros((X.shape[0] -h +1 ,X.shape[1]-w + 1)) # 初始化输出矩阵，全为0\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):# Y中每个元素都做一次互相关运算\n",
    "            Y[i,j] = (X[i:i+h,j:j+w]*K).sum() # i,j元素值=取此处核大小个矩阵，与K做点积，求和\n",
    "    return Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 验证一下上面的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]])\n",
    "K = torch.tensor([[0.0,1.0],[2.0,3.0]])\n",
    "corr2d(X,K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己写个卷积层\n",
    "\n",
    "- 基于上面定义的`corr2d`函数[**实现二维卷积层**]。\n",
    "- 在`__init__`构造函数中，将`weight`和`bias`声明为两个模型参数。\n",
    "  - 这两个也是需要训练的参数，用`nn.Parameter()`声明,同时进行初始化\n",
    "- 前向传播函数调用`corr2d`函数并添加偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self,x):\n",
    "        return corr2d(x,self.weight) + self.bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 高度和宽度分别为$h$和$w$的卷积核可以被称为$h \\times w$卷积或$h \\times w$卷积核。\n",
    "- 我们也将带有$h \\times w$卷积核的卷积层称为$h \\times w$卷积层。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用一下：检测图像中的边缘\n",
    "- 自己构造一个黑白图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自己构造一个高度1，宽度2 的卷积核K\n",
    "- 当进行互相关运算时，如果水平相邻的两元素相同，则输出为0，否则输出为非0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0,-1.0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在，我们对参数`X`（输入）和`K`（卷积核）执行互相关运算。\n",
    "- 如下所示，[**输出`Y`中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘**]，其他情况的输出为$0$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X,K)\n",
    "Y # Y得到输出，即得到边缘。非0值表示边缘"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果图像转置之后，用这个卷积核，会发现不行\n",
    "- 这个[**卷积核`K`只可以检测垂直边缘**]，无法检测水平边缘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(),K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上面是自己设置的一个卷积核，接下来自己来学习出卷积核\n",
    "- 当不知道卷积核数值应该为多少时，就要通过学习得到\n",
    "- 卷积核大小和维度，大多依据经验，或者多组实验然后取最好的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2,loss7.514\n",
      "epoch4,loss1.637\n",
      "epoch6,loss0.429\n",
      "epoch8,loss0.135\n",
      "epoch10,loss0.049\n"
     ]
    }
   ],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为(1,2)的卷积核\n",
    "conv2d = nn.Conv2d(1,1,kernel_size = (1,2),bias=False)\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度）\n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1,1,6,8))\n",
    "Y = Y.reshape(1,1,6,7) # 输出y的大小又公式计算得出\n",
    "lr = 3e-2 # 学习率\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat -Y )**2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # 迭代卷积核,即手动更新参数，这里没有bias\n",
    "    conv2d.weight.data[:] -= lr*conv2d.weight.grad\n",
    "    if (i+1) % 2 == 0 :\n",
    "        print(f\"epoch{i+1},loss{l.sum():.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在$10$次迭代之后，误差已经降到足够低。现在我们来看看我们[**所学的卷积核的权重张量**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9657, -1.0089]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输入通道 从零实现\n",
    "- 即每个输入通道执行互相关操作，然后将结果相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "def corr2d_multi_in(X,K):\n",
    "    # 先遍历X和K的第0个维度（通道维度），再把他们加在一起\n",
    "    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 验证一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]],\n",
    "                  [[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\n",
    "\n",
    "K = torch.tensor([[[0.0,1.0],[2.0,3.0]],\n",
    "                  [[1.0,2.0],[3.0,4.0]]])\n",
    "\n",
    "corr2d_multi_in(X,K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多通道输出\n",
    "- $c_i$和$c_o$分别表示输入和输出通道的数目\n",
    "- 如下所示，我们实现一个[**计算多个通道的输出的互相关函数**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X,K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算\n",
    "    # 最后将所有结果都叠加在一起 \n",
    "    # torch.stack实现 ：按第0维度叠加一起\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过将核张量`K`与`K+1`（`K`中每个元素加$1$）和`K+2`连接起来，构造了一个具有$3$个输出通道的卷积核。\n",
    "- 三个卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2, 2]),\n",
       " tensor([[[[0., 1.],\n",
       "           [2., 3.]],\n",
       " \n",
       "          [[1., 2.],\n",
       "           [3., 4.]]],\n",
       " \n",
       " \n",
       "         [[[1., 2.],\n",
       "           [3., 4.]],\n",
       " \n",
       "          [[2., 3.],\n",
       "           [4., 5.]]],\n",
       " \n",
       " \n",
       "         [[[2., 3.],\n",
       "           [4., 5.]],\n",
       " \n",
       "          [[3., 4.],\n",
       "           [5., 6.]]]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K,K+1,K+2),0)\n",
    "K.shape ,K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三个卷积核-》三个输出\n",
    "- 下面，我们对输入张量`X`与卷积核张量`K`执行互相关运算。现在的输出包含$3$个通道，第一个通道的结果与先前输入张量`X`和多输入单输出通道的结果一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1卷积核\n",
    "- 用的挺多\n",
    "- 不识别空间信息\n",
    "  - 因为只看每个通道的一个像素，不看周围的\n",
    "- 只融合通道，进行降维\n",
    "  - 不同通道，每个像素，按照1x1卷积核对应的数值（权值），进行加权求和的运算\n",
    "- 相当于全连接层\n",
    "  - 输入是，每个通道的所有像素（可以把每个通道拉成一维向量）\n",
    "  - 权重为 维度为输入通道数x输出通道数的矩阵\n",
    "    - 矩阵的值是卷积核的值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "* 多输入多输出通道可以用来扩展卷积层的模型。\n",
    "* 当以每像素为基础应用时，$1\\times 1$卷积层相当于全连接层。\n",
    "* $1\\times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_pytorch",
   "language": "python",
   "name": "d2l_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
