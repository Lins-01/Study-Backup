{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我这边能到 0.74082。但是可惜的是最初没有固定随机种子。没想到随手一发是巅峰了。现在没算力和时间了，希望能对大家有所帮助。根据之前的比赛经验，一个好的backbone似乎能起到很大的作用。所以我就选了大的。但是因为到paddle不是十分熟悉。就选用了。ernie_vil-2.0-base-zh。 然后使用这个替换到原始的文本和图像backbone。 在融合部分，根据之前的经验，这部分其实不是十分重要，就直接拼接起来送入一个层数较少的bert。最后接了一个简单的线性分类头。\n",
    "还有就是baseline的路径有点问题，修复了一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023IKCEST第五届“一带一路”国际大数据竞赛\n",
    "# 一、背景介绍\n",
    "\n",
    "本届大数据竞赛在中国工程院、教育部高等学校大学计算机课程教学指导委员会及丝绸之路大学联盟的指导下由联合国教科文组织国际工程科技知识中心（IKCEST）、中国工程科技知识中心（CKCEST）、百度公司及西安交通大学共同主办，旨在放眼“一带一路”倡议沿线国家，通过竞赛方式挖掘全球大数据人工智能尖端人才，实现政府—产业—高校合力推动大数据产业研究、应用、发展的目标，进一步夯实赛事的理论基础与实践基础，加快拔尖AI创新人才培养。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、赛题介绍\n",
    "随着新媒体时代信息媒介的多元化发展，各种内容大量活跃在媒体内中，与此同时各类虚假信息也充斥着社交媒体，影响着公众的判断和决策。如何在大量的文本、图像等多模态信息中，通过大数据与人工智能技术，纠正和消除虚假错误信息，对于网络舆情及社会治理有着重大意义。\n",
    "\n",
    "本次赛题要求选手基于官方指定数据集，通过建模同一事实跨模态数据之间的关系 （主要是文本和图像），实现对任一模态信息能够进行虚假和真实性的检测。鼓励参赛选手通过大模型解决问题，进行技术探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:42.588960Z",
     "iopub.status.busy": "2023-09-06T05:11:42.588526Z",
     "iopub.status.idle": "2023-09-06T05:11:45.503125Z",
     "shell.execute_reply": "2023-09-06T05:11:45.502228Z",
     "shell.execute_reply.started": "2023-09-06T05:11:42.588931Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlenlp==2.4.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: paddle2onnx in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (1.0.6)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (0.70.12.2)\n",
      "Requirement already satisfied: dill<0.3.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (0.3.4)\n",
      "Requirement already satisfied: colorlog in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (6.7.0)\n",
      "Requirement already satisfied: paddlefsl in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (1.1.0)\n",
      "Requirement already satisfied: sentencepiece in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (0.1.99)\n",
      "Requirement already satisfied: seqeval in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (1.2.2)\n",
      "Requirement already satisfied: colorama in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (0.4.6)\n",
      "Requirement already satisfied: jieba in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (0.42.1)\n",
      "Requirement already satisfied: tqdm in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (4.65.0)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (3.20.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (2.13.1)\n",
      "Requirement already satisfied: visualdl in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddlenlp==2.4.2) (2.4.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (2023.1.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (6.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (6.0.1)\n",
      "Requirement already satisfied: packaging in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (1.21.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (0.16.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (12.0.1)\n",
      "Requirement already satisfied: pandas in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (1.3.5)\n",
      "Requirement already satisfied: aiohttp in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (3.8.5)\n",
      "Requirement already satisfied: xxhash in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (3.2.0)\n",
      "Requirement already satisfied: six in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from paddle2onnx->paddlenlp==2.4.2) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from seqeval->paddlenlp==2.4.2) (1.0.2)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp==2.4.2) (9.2.0)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp==2.4.2) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp==2.4.2) (3.5.3)\n",
      "Requirement already satisfied: flask>=1.1.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp==2.4.2) (2.2.5)\n",
      "Requirement already satisfied: bce-python-sdk in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from visualdl->paddlenlp==2.4.2) (0.8.87)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.2.3)\n",
      "Requirement already satisfied: click>=8.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (8.1.6)\n",
      "Requirement already satisfied: Jinja2>=3.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (3.1.2)\n",
      "Requirement already satisfied: pytz in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.4.2) (2023.3)\n",
      "Requirement already satisfied: Babel>=2.3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.4.2) (2.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (4.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (23.1.0)\n",
      "Requirement already satisfied: filelock in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets>=2.0.0->paddlenlp==2.4.2) (3.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.4.2) (3.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (1.3.1)\n",
      "Requirement already satisfied: future>=0.6.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from bce-python-sdk->visualdl->paddlenlp==2.4.2) (0.18.3)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from bce-python-sdk->visualdl->paddlenlp==2.4.2) (3.18.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\codeapp\\anaconda\\envs\\paddle\\lib\\site-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "#环境安装\n",
    "# ! pip install paddlenlp==2.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、数据集介绍\n",
    "本次比赛提供从国内外主流社交媒体平台上爬取的含有不同领域声明的数据集。\n",
    "\n",
    "初赛：训练集与验证集： 提供中文训练集5694条以及英文数据4893条，同时公开英文验证集611条与中文验证集711条供选手优化模型。\n",
    "\n",
    "初赛评测数据： 提供文娱、经济、健康领域的测试数据，这些领域的数据较容易区分。英文与中文数据集的测试集各600条。参赛队伍上传的结果文本的每一行就是对应的分类结果，该数据不公布，用于评测。\n",
    "\n",
    "\n",
    "| 0 | 1 | 2 |\n",
    "| -------- | -------- | -------- |\n",
    "| non-rumor | rumor  | unverified |\n",
    "\n",
    "\n",
    "\n",
    "[复赛数据后续见官网通知](https://aistudio.baidu.com/aistudio/competition/detail/1030/0/task-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、数据预处理\n",
    "**数据集过大，右键选择解压/home/aistudio/data/data229919/data.zip数据集，耐心等待30分钟，直到出现以下文件夹和文件,解压之后硬盘达到约80g（压缩包27g、解压文件之后50g，可以将项目挂载的数据集取消，空余出27g）**\n",
    "* test\n",
    "* train\n",
    "* val\n",
    "* dataset_items_test.json\n",
    "* dataset_items_train.json\n",
    "* dataset_items_val.json\n",
    "\n",
    "此处将数据集已经放置在queries_dataset_merge文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:45.505190Z",
     "iopub.status.busy": "2023-09-06T05:11:45.504826Z",
     "iopub.status.idle": "2023-09-06T05:11:48.059038Z",
     "shell.execute_reply": "2023-09-06T05:11:48.058228Z",
     "shell.execute_reply.started": "2023-09-06T05:11:45.505163Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddle.fluid.layers.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12868\\3804492379.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddlenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;34m\"Please import paddlenlp before datasets module to avoid download issues\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     )\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcollate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata_collator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\data\\data_collator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer_utils_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPretrainedTokenizerBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m __all__ = [\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodel_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPretrainedModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregister_base_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtokenizer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPretrainedTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBPETokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_chinese_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize_special_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProcessorMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\model_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddlenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInitTrackerMeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn_args_to_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madapt_stale_fwd_patch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon_ops_import\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfluid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmap_structure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddlenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paddle.fluid.layers.utils'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:48.060567Z",
     "iopub.status.busy": "2023-09-06T05:11:48.060105Z",
     "iopub.status.idle": "2023-09-06T05:11:48.106969Z",
     "shell.execute_reply": "2023-09-06T05:11:48.106366Z",
     "shell.execute_reply.started": "2023-09-06T05:11:48.060538Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "import json\n",
    "data_items_train = json.load(open(r\"E:\\Document\\CodeSpace\\Data_set\\Paddle2023IKCEST\\queries_dataset_merge\\dataset_items_train.json\"))\n",
    "data_items_val = json.load(open(r\"E:\\Document\\CodeSpace\\Data_set\\Paddle2023IKCEST\\queries_dataset_merge\\dataset_items_val.json\"))\n",
    "data_items_test = json.load(open(r\"E:\\Document\\CodeSpace\\Data_set\\Paddle2023IKCEST\\queries_dataset_merge\\dataset_items_test.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据中的每一个样本：图像img、文本caption、对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:48.108325Z",
     "iopub.status.busy": "2023-09-06T05:11:48.107976Z",
     "iopub.status.idle": "2023-09-06T05:11:48.132189Z",
     "shell.execute_reply": "2023-09-06T05:11:48.131610Z",
     "shell.execute_reply.started": "2023-09-06T05:11:48.108301Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "import os \n",
    "import imghdr\n",
    "\n",
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>','')\n",
    "    input_str = input_str.replace('</b>','')\n",
    "    #input_str = unidecode(input_str)  \n",
    "    return input_str\n",
    "    \n",
    "class NewsContextDatasetEmbs(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split):\n",
    "        self.context_data_items_dict = context_data_items_dict\n",
    "        self.queries_root_dir = queries_root_dir\n",
    "        self.idx_to_keys = list(context_data_items_dict.keys())\n",
    "        self.transform =T.Compose([\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "        self.split=split\n",
    "    def __len__(self):\n",
    "        return len(self.context_data_items_dict)   \n",
    "\n",
    "\n",
    "    def load_img_pil(self,image_path):\n",
    "        if imghdr.what(image_path) == 'gif': \n",
    "            try:\n",
    "                with open(image_path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('RGB')\n",
    "            except:\n",
    "                return None \n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "    def load_imgs_direct_search(self,item_folder_path,direct_dict):   \n",
    "        list_imgs_tensors = []\n",
    "        count = 0   \n",
    "        keys_to_check = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys_to_check:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    image_path = os.path.join(item_folder_path,page['image_path'].split('/')[-1])\n",
    "                    try:\n",
    "                        pil_img = self.load_img_pil(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(image_path)\n",
    "                    if pil_img == None: continue\n",
    "                    transform_img = self.transform(pil_img)\n",
    "                    count = count + 1 \n",
    "                    list_imgs_tensors.append(transform_img)\n",
    "        stacked_tensors = paddle.stack(list_imgs_tensors, axis=0)\n",
    "        return stacked_tensors\n",
    "    def load_captions(self,inv_dict):\n",
    "        captions = ['']\n",
    "        pages_with_captions_keys = ['all_fully_matched_captions','all_partially_matched_captions']\n",
    "        for key1 in pages_with_captions_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        item = page['title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    \n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "                    \n",
    "        pages_with_title_only_keys = ['partially_matched_no_text','fully_matched_no_text']\n",
    "        for key1 in pages_with_title_only_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        title = process_string(page['title'])\n",
    "                        captions.append(title)\n",
    "        return captions\n",
    "\n",
    "    def load_captions_weibo(self,direct_dict):\n",
    "        captions = ['']\n",
    "        keys = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    if 'page_title' in page.keys():\n",
    "                        item = page['page_title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "        #print(captions)\n",
    "        return captions\n",
    "        #加载img文件夹\n",
    "    def load_queries(self,key):\n",
    "        caption = self.context_data_items_dict[key]['caption']\n",
    "        image_path = os.path.join(self.queries_root_dir,self.context_data_items_dict[key]['image_path'])\n",
    "        pil_img = self.load_img_pil(image_path)\n",
    "        transform_img = self.transform(pil_img)\n",
    "        return transform_img, caption\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        #print(self.context_data_items_dict)      \n",
    "        #idx = idx.tolist()               \n",
    "        key = self.idx_to_keys[idx]\n",
    "        #print(key)\n",
    "        item=self.context_data_items_dict.get(str(key))\n",
    "        #print(item)\n",
    "        # 如果为test没有label属性\n",
    "        #print(self.split)\n",
    "        if self.split=='train' or self.split=='val':\n",
    "            label = paddle.to_tensor(int(item['label']))\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json')))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json')))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'label': label, 'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        else:\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json')))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json')))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)     \n",
    "            qImg,qCap =  self.load_queries(key)\n",
    "            sample = {'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap}\n",
    "        #print(sample)\n",
    "        #print(len(captions)) \n",
    "        #print(type(imgs))\n",
    "        #print(imgs.size)\n",
    "        #print(imgs.shape)  \n",
    "        return sample,  len(captions), imgs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:48.134434Z",
     "iopub.status.busy": "2023-09-06T05:11:48.134126Z",
     "iopub.status.idle": "2023-09-06T05:11:48.137708Z",
     "shell.execute_reply": "2023-09-06T05:11:48.137145Z",
     "shell.execute_reply.started": "2023-09-06T05:11:48.134413Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### load Datasets ####\n",
    "train_dataset = NewsContextDatasetEmbs(data_items_train, r'E:\\Document\\CodeSpace\\Data_set\\Paddle2023IKCEST\\queries_dataset_merge','train')\n",
    "val_dataset = NewsContextDatasetEmbs(data_items_val,r'E:\\Document\\CodeSpace\\Data_set\\Paddle2023IKCEST\\queries_dataset_merge','val')\n",
    "test_dataset = NewsContextDatasetEmbs(data_items_test,r'E:\\Document\\CodeSpace\\Data_set\\Paddle2023IKCEST\\queries_dataset_merge','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:48.138768Z",
     "iopub.status.busy": "2023-09-06T05:11:48.138476Z",
     "iopub.status.idle": "2023-09-06T05:11:49.653409Z",
     "shell.execute_reply": "2023-09-06T05:11:49.652707Z",
     "shell.execute_reply.started": "2023-09-06T05:11:48.138748Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'caption': ['', '', 'Boston Orange  波士頓菊子: 朱学渊  - 為中國史學的實證化而努力', '新华每日电讯-微报纸-2022年01月28日', '新华每日电讯-微报纸-2021年11月19日'], 'imgs': Tensor(shape=[3, 3, 224, 224], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[[ 1.71804118,  1.47829461,  1.51254416, ..., -1.07329392,\n",
      "           -1.03904438, -1.05616915],\n",
      "          [ 1.39267087,  1.37554610,  1.58104312, ..., -1.03904438,\n",
      "           -1.05616915, -1.07329392],\n",
      "          [ 1.42692029,  1.47829461,  1.66666687, ..., -1.00479496,\n",
      "           -1.02191973, -1.02191973],\n",
      "          ...,\n",
      "          [-1.72403467, -1.74115944, -1.77540886, ...,  0.57068247,\n",
      "            0.50218344,  0.38231018],\n",
      "          [-1.74115944, -1.65553570, -1.63841093, ...,  0.51930821,\n",
      "            0.43368444,  0.21106265],\n",
      "          [-1.75828421, -1.75828421, -1.70690989, ...,  0.51930821,\n",
      "            0.41655967,  0.12543888]],\n",
      "\n",
      "         [[ 1.58823562,  1.36064458,  1.37815154, ..., -1.09033608,\n",
      "           -1.09033608, -1.12535000],\n",
      "          [ 1.22058833,  1.23809528,  1.44817960, ..., -1.09033608,\n",
      "           -1.10784304, -1.12535000],\n",
      "          [ 1.20308125,  1.30812323,  1.53571451, ..., -1.05532205,\n",
      "           -1.07282901, -1.09033608],\n",
      "          ...,\n",
      "          [-1.65056014, -1.66806722, -1.68557417, ...,  1.02801120,\n",
      "            0.87044823,  0.62535024],\n",
      "          [-1.68557417, -1.58053207, -1.56302512, ...,  0.97549027,\n",
      "            0.80042022,  0.45028022],\n",
      "          [-1.72058821, -1.68557417, -1.63305318, ...,  0.97549027,\n",
      "            0.78291327,  0.36274520]],\n",
      "\n",
      "         [[ 1.35023987,  1.12366033,  1.14108944, ..., -0.77612191,\n",
      "           -0.75869268, -0.79355109],\n",
      "          [ 1.00165594,  1.00165594,  1.21080625, ..., -0.77612191,\n",
      "           -0.79355109, -0.81098026],\n",
      "          [ 1.00165594,  1.08880186,  1.29795229, ..., -0.74126351,\n",
      "           -0.75869268, -0.77612191],\n",
      "          ...,\n",
      "          [-1.49071896, -1.49071896, -1.52557731, ...,  1.10623109,\n",
      "            0.96679759,  0.74021804],\n",
      "          [-1.50814819, -1.42100215, -1.40357304, ...,  1.05394351,\n",
      "            0.89708078,  0.60078448],\n",
      "          [-1.54300654, -1.52557731, -1.47328973, ...,  1.03651428,\n",
      "            0.87965161,  0.51363856]]],\n",
      "\n",
      "\n",
      "        [[[ 2.24890828,  2.24890828,  2.24890828, ...,  2.19753432,\n",
      "            2.24890828,  2.24890828],\n",
      "          [ 2.24890828,  2.24890828,  2.24890828, ...,  2.21465898,\n",
      "            2.24890828,  2.24890828],\n",
      "          [ 2.24890828,  2.24890828,  2.24890828, ...,  2.19753432,\n",
      "            2.24890828,  2.24890828],\n",
      "          ...,\n",
      "          [-1.39866436, -1.39866436, -1.39866436, ...,  1.94066298,\n",
      "            1.68379164,  1.75229061],\n",
      "          [ 1.28992236,  1.28992236,  1.28992236, ...,  1.78654015,\n",
      "            2.14615989,  1.83791447],\n",
      "          [ 2.19753432,  2.19753432,  2.19753432, ...,  1.52966881,\n",
      "            1.76941538,  1.59816790]],\n",
      "\n",
      "         [[ 2.42857146,  2.42857146,  2.42857146, ...,  2.37605071,\n",
      "            2.42857146,  2.42857146],\n",
      "          [ 2.42857146,  2.42857146,  2.42857146, ...,  2.39355755,\n",
      "            2.42857146,  2.42857146],\n",
      "          [ 2.42857146,  2.42857146,  2.42857146, ...,  2.37605071,\n",
      "            2.42857146,  2.42857146],\n",
      "          ...,\n",
      "          [ 0.03011219,  0.03011219,  0.03011219, ...,  2.11344552,\n",
      "            1.85084057,  1.92086864],\n",
      "          [ 1.78081262,  1.78081262,  1.78081262, ...,  1.95588255,\n",
      "            2.32352948,  2.00840354],\n",
      "          [ 2.41106486,  2.41106486,  2.41106486, ...,  1.69327760,\n",
      "            1.93837559,  1.76330554]],\n",
      "\n",
      "         [[ 2.64000010,  2.64000010,  2.64000010, ...,  2.58771276,\n",
      "            2.64000010,  2.64000010],\n",
      "          [ 2.64000010,  2.64000010,  2.64000010, ...,  2.60514212,\n",
      "            2.64000010,  2.64000010],\n",
      "          [ 2.64000010,  2.64000010,  2.64000010, ...,  2.58771276,\n",
      "            2.64000010,  2.64000010],\n",
      "          ...,\n",
      "          [ 1.31538141,  1.29795229,  1.33281064, ...,  2.32627511,\n",
      "            2.06483698,  2.13455367],\n",
      "          [ 2.23912883,  2.23912883,  2.23912883, ...,  2.16941214,\n",
      "            2.53542542,  2.22169971],\n",
      "          [ 2.55285430,  2.55285430,  2.55285430, ...,  1.90797424,\n",
      "            2.15198302,  1.97769105]]],\n",
      "\n",
      "\n",
      "        [[[ 2.24890828,  2.24890828,  2.24890828, ...,  1.47829461,\n",
      "            1.85503912,  1.63241732],\n",
      "          [ 2.24890828,  2.24890828,  2.24890828, ...,  1.97491241,\n",
      "            2.00916195,  2.07766104],\n",
      "          [ 2.24890828,  2.24890828,  2.24890828, ...,  1.54679358,\n",
      "            1.47829461,  1.71804118],\n",
      "          ...,\n",
      "          [ 1.75229061,  1.82078969,  1.75229061, ...,  2.23178363,\n",
      "            2.23178363,  2.23178363],\n",
      "          [ 2.14615989,  2.02628660,  2.00916195, ...,  1.85503912,\n",
      "            1.82078969,  1.82078969],\n",
      "          [ 1.95778763,  1.61529267,  1.64954209, ...,  1.37554610,\n",
      "            1.42692029,  1.39267087]],\n",
      "\n",
      "         [[ 2.42857146,  2.42857146,  2.42857146, ...,  1.64075661,\n",
      "            2.02591062,  1.79831958],\n",
      "          [ 2.42857146,  2.42857146,  2.42857146, ...,  2.14845967,\n",
      "            2.18347359,  2.25350142],\n",
      "          [ 2.42857146,  2.42857146,  2.42857146, ...,  1.71078455,\n",
      "            1.64075661,  1.88585460],\n",
      "          ...,\n",
      "          [ 1.92086864,  1.99089658,  1.92086864, ...,  2.41106486,\n",
      "            2.41106486,  2.41106486],\n",
      "          [ 2.32352948,  2.20098066,  2.18347359, ...,  2.02591062,\n",
      "            1.99089658,  1.99089658],\n",
      "          [ 2.13095260,  1.78081262,  1.81582654, ...,  1.53571451,\n",
      "            1.58823562,  1.55322158]],\n",
      "\n",
      "         [[ 2.64000010,  2.64000010,  2.64000010, ...,  1.85568666,\n",
      "            2.23912883,  2.01254940],\n",
      "          [ 2.64000010,  2.64000010,  2.64000010, ...,  2.36113334,\n",
      "            2.39599180,  2.46570849],\n",
      "          [ 2.64000010,  2.64000010,  2.64000010, ...,  1.92540348,\n",
      "            1.85568666,  2.09969544],\n",
      "          ...,\n",
      "          [ 2.13455367,  2.20427060,  2.13455367, ...,  2.62257099,\n",
      "            2.62257099,  2.62257099],\n",
      "          [ 2.53542542,  2.41342068,  2.39599180, ...,  2.23912883,\n",
      "            2.20427060,  2.20427060],\n",
      "          [ 2.34370399,  1.99512029,  2.02997851, ...,  1.75111151,\n",
      "            1.80339909,  1.76854074]]]]), 'qImg': Tensor(shape=[3, 224, 224], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[ 2.23178363,  1.68379164,  1.85503912, ...,  2.24890828,\n",
      "           2.24890828,  2.24890828],\n",
      "         [ 2.11191034,  1.66666687,  1.88928866, ...,  2.24890828,\n",
      "           2.24890828,  2.24890828],\n",
      "         [ 1.94066298,  1.68379164,  1.87216389, ...,  2.24890828,\n",
      "           2.24890828,  2.24890828],\n",
      "         ...,\n",
      "         [ 0.36518541,  1.99203718,  0.09118938, ...,  0.43368444,\n",
      "           1.10154974,  0.67343098],\n",
      "         [ 0.36518541,  1.80366492,  0.02269037, ...,  1.63241732,\n",
      "           1.76941538, -0.02868389],\n",
      "         [ 0.29668641,  0.63918144,  0.38231018, ...,  0.58780718,\n",
      "           1.49541938, -0.26843041]],\n",
      "\n",
      "        [[ 2.09593868, -0.47759098, -1.96568620, ...,  2.42857146,\n",
      "           2.42857146,  2.42857146],\n",
      "         [ 1.58823562, -1.33543408, -1.98319328, ...,  2.42857146,\n",
      "           2.42857146,  2.42857146],\n",
      "         [ 0.69537824, -1.77310920, -1.98319328, ...,  2.42857146,\n",
      "           2.42857146,  2.42857146],\n",
      "         ...,\n",
      "         [ 0.50280124,  2.16596651,  0.22268920, ...,  0.57282925,\n",
      "           1.25560224,  0.81792724],\n",
      "         [ 0.50280124,  1.97338963,  0.15266119, ...,  1.79831958,\n",
      "           1.93837559,  0.10014019],\n",
      "         [ 0.43277320,  0.78291327,  0.52030820, ...,  0.73039222,\n",
      "           1.65826356, -0.14495783]],\n",
      "\n",
      "        [[ 2.44827914, -0.06152484, -1.40357304, ...,  2.64000010,\n",
      "           2.64000010,  2.64000010],\n",
      "         [ 2.02997851, -0.88069707, -1.47328973, ...,  2.64000010,\n",
      "           2.64000010,  2.64000010],\n",
      "         [ 1.10623109, -1.29899776, -1.47328973, ...,  2.64000010,\n",
      "           2.64000010,  2.64000010],\n",
      "         ...,\n",
      "         [ 0.72278887,  2.37856245,  0.44392177, ...,  0.79250562,\n",
      "           1.47224414,  1.03651428],\n",
      "         [ 0.72278887,  2.18684125,  0.37420499, ...,  2.01254940,\n",
      "           2.15198302,  0.32191741],\n",
      "         [ 0.65307206,  1.00165594,  0.74021804, ...,  0.94936836,\n",
      "           1.87311590,  0.07790870]]]), 'qCap': '看到有人说 这老头说了句话 不是我退休了 要是没退休 你早就在牢里了 说是某地政法系统的前领导 正局级干部退休的 我想问这种人敢说出这种话 在职间到底'}, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "# 打印数据\n",
    "for step, batch in enumerate(test_dataset, start=1):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:49.654785Z",
     "iopub.status.busy": "2023-09-06T05:11:49.654468Z",
     "iopub.status.idle": "2023-09-06T05:11:49.667913Z",
     "shell.execute_reply": "2023-09-06T05:11:49.667309Z",
     "shell.execute_reply.started": "2023-09-06T05:11:49.654762Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle \n",
    "def collate_context_bert_train(batch):\n",
    "    #print(batch)\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    labels = [] \n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        labels.append(sample['label'])\n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0], sample['imgs'].shape[1], sample['imgs'].shape[2], sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        #print(1)\n",
    "        img_batch.append(padded_mem_img)#pad证据图片\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])#[3, 224, 224]\n",
    "        qCap_batch.append(sample['qCap'])     \n",
    "    #print(labels)   \n",
    "    #print(img_batch)\n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    labels = paddle.stack(labels, axis=0) \n",
    "    #print(3)  \n",
    "    return labels, cap_batch, img_batch, qCap_batch, qImg_batch\n",
    "\n",
    "def collate_context_bert_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    cap_batch = []\n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1],sample['imgs'].shape[2],sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        img_batch.append(padded_mem_img)\n",
    "        cap_batch.append(captions)\n",
    "        qImg_batch.append(sample['qImg'])\n",
    "        qCap_batch.append(sample['qCap'])        \n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    return cap_batch, img_batch, qCap_batch, qImg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:49.668989Z",
     "iopub.status.busy": "2023-09-06T05:11:49.668708Z",
     "iopub.status.idle": "2023-09-06T05:11:49.673520Z",
     "shell.execute_reply": "2023-09-06T05:11:49.672941Z",
     "shell.execute_reply.started": "2023-09-06T05:11:49.668968Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load DataLoader\n",
    "from paddle.io import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn = collate_context_bert_train, return_list=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn = collate_context_bert_train,  return_list=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn = collate_context_bert_test, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:11:49.674591Z",
     "iopub.status.busy": "2023-09-06T05:11:49.674313Z",
     "iopub.status.idle": "2023-09-06T05:12:02.782508Z",
     "shell.execute_reply": "2023-09-06T05:12:02.778536Z",
     "shell.execute_reply.started": "2023-09-06T05:11:49.674570Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [2]), [['', 'Six Of Crows Boxed Set - By  Leigh Bardugo (mixed Media Product) : Target', 'Six of Crows Boxed Set - by  Leigh Bardugo (Mixed Media Product), image 1 of 2 slides', 'Amazon.com', '', 'El secreto para conseguir las mejores ofertas en Amazon - Infobae', 'Ofertas del día en Amazon', 'Amazon.com - Ofertas del día', \"Amazon.com - Today's Deals\"]], Tensor(shape=[1, 3, 3, 224, 224], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[[[ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.11191034,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.21465898,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           ...,\n",
      "           [ 2.06053615,  2.06053615,  2.06053615, ...,  0.51930821,\n",
      "             0.65630621,  1.01592600],\n",
      "           [ 2.04341149,  2.04341149,  2.04341149, ...,  0.58780718,\n",
      "             0.86180323,  1.20429862],\n",
      "           [ 2.02628660,  2.02628660,  2.02628660, ...,  1.61529267,\n",
      "             1.73516595,  1.78654015]],\n",
      "\n",
      "          [[ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.28851557,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.39355755,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           ...,\n",
      "           [ 1.62324953,  1.62324953,  1.62324953, ...,  0.25770321,\n",
      "             0.43277320,  0.83543426],\n",
      "           [ 1.57072854,  1.57072854,  1.57072854, ...,  0.29271722,\n",
      "             0.53781521,  0.97549027],\n",
      "           [ 1.53571451,  1.53571451,  1.53571451, ...,  1.22058833,\n",
      "             1.22058833,  1.34313750]],\n",
      "\n",
      "          [[ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.50056696,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.60514212,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           ...,\n",
      "           [ 0.32191741,  0.32191741,  0.32191741, ..., -0.42753804,\n",
      "            -0.35782126,  0.04305032],\n",
      "           [ 0.19991305,  0.19991305,  0.19991305, ..., -0.49725482,\n",
      "            -0.44496724, -0.06152484],\n",
      "           [ 0.07790870,  0.07790870,  0.07790870, ...,  0.02562112,\n",
      "            -0.13124162,  0.06047951]]],\n",
      "\n",
      "\n",
      "         [[[ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           ...,\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828]],\n",
      "\n",
      "          [[ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           ...,\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146]],\n",
      "\n",
      "          [[ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           ...,\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010]]],\n",
      "\n",
      "\n",
      "         [[[ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           ...,\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828],\n",
      "           [ 2.24890828,  2.24890828,  2.24890828, ...,  2.24890828,\n",
      "             2.24890828,  2.24890828]],\n",
      "\n",
      "          [[ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           ...,\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146],\n",
      "           [ 2.42857146,  2.42857146,  2.42857146, ...,  2.42857146,\n",
      "             2.42857146,  2.42857146]],\n",
      "\n",
      "          [[ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           ...,\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010],\n",
      "           [ 2.64000010,  2.64000010,  2.64000010, ...,  2.64000010,\n",
      "             2.64000010,  2.64000010]]]]]), ['Oferta Amazon '], Tensor(shape=[1, 3, 224, 224], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[[2.24890828, 2.24890828, 2.24890828, ..., 2.24890828,\n",
      "           2.24890828, 2.24890828],\n",
      "          [2.24890828, 2.24890828, 2.24890828, ..., 2.24890828,\n",
      "           2.24890828, 2.24890828],\n",
      "          [2.24890828, 2.24890828, 2.24890828, ..., 2.24890828,\n",
      "           2.24890828, 2.24890828],\n",
      "          ...,\n",
      "          [2.24890828, 2.24890828, 2.24890828, ..., 2.24890828,\n",
      "           2.24890828, 2.24890828],\n",
      "          [2.24890828, 2.24890828, 2.24890828, ..., 2.24890828,\n",
      "           2.24890828, 2.24890828],\n",
      "          [2.24890828, 2.24890828, 2.24890828, ..., 2.24890828,\n",
      "           2.24890828, 2.24890828]],\n",
      "\n",
      "         [[2.42857146, 2.42857146, 2.42857146, ..., 2.42857146,\n",
      "           2.42857146, 2.42857146],\n",
      "          [2.42857146, 2.42857146, 2.42857146, ..., 2.42857146,\n",
      "           2.42857146, 2.42857146],\n",
      "          [2.42857146, 2.42857146, 2.42857146, ..., 2.42857146,\n",
      "           2.42857146, 2.42857146],\n",
      "          ...,\n",
      "          [2.42857146, 2.42857146, 2.42857146, ..., 2.42857146,\n",
      "           2.42857146, 2.42857146],\n",
      "          [2.42857146, 2.42857146, 2.42857146, ..., 2.42857146,\n",
      "           2.42857146, 2.42857146],\n",
      "          [2.42857146, 2.42857146, 2.42857146, ..., 2.42857146,\n",
      "           2.42857146, 2.42857146]],\n",
      "\n",
      "         [[2.64000010, 2.64000010, 2.64000010, ..., 2.64000010,\n",
      "           2.64000010, 2.64000010],\n",
      "          [2.64000010, 2.64000010, 2.64000010, ..., 2.64000010,\n",
      "           2.64000010, 2.64000010],\n",
      "          [2.64000010, 2.64000010, 2.64000010, ..., 2.64000010,\n",
      "           2.64000010, 2.64000010],\n",
      "          ...,\n",
      "          [2.64000010, 2.64000010, 2.64000010, ..., 2.64000010,\n",
      "           2.64000010, 2.64000010],\n",
      "          [2.64000010, 2.64000010, 2.64000010, ..., 2.64000010,\n",
      "           2.64000010, 2.64000010],\n",
      "          [2.64000010, 2.64000010, 2.64000010, ..., 2.64000010,\n",
      "           2.64000010, 2.64000010]]]])]\n"
     ]
    }
   ],
   "source": [
    "# 打印数据\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型构建\n",
    "**本次赛题为一个NLP与多模态的分类赛题，整体建模采用特征提取、特征交互、预测分类三个阶段**\n",
    "\n",
    "**特征提取：** 对于图像数据，使用ResNet模型进行特征提取、对于文本数据，使用预训练模型Ernie-m多语言模型对中文和英文同时处理，qCap,qImg,（需要验证的标题或图像材料）、caps,imgs（支持验证的文本、图像证据材料）\n",
    "\n",
    "**特征交互**：使用多头自注意力机制，将标题与文本证据材料交互、图像与图像证据材料交互，输出与需要验证的标题和图像的相关证据特征caps_feature、imgs_features\n",
    "\n",
    "**预测分类：** 最后使用全连接层将标题特征、图像特征、相关的文本证据特征、相关的图像证据特征拼接输入到分类器得到最终结果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f29e3f853b9445fbeb24189103cdbbcb8364498dc484593a891839994dadbd6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多语言预训练模型ERNIE-M\n",
    "2021年，百度发布多语言预训练模型ERNIE-M。ERNIE-M通过对96门语言的学习，使得一个模型能同时理解96种语言，该项技术在5类典型跨语言理解任务上刷新世界最好效果。\n",
    "\n",
    "## ERNIE-M原理\n",
    "ERNIE-M基于飞桨PaddlePaddle框架训练，该模型构建了大小为25万的多语言词表，涵盖了96种语言的大多数常见词汇，训练语料包含了汉语、英语、法语、南非语、阿尔巴尼亚语、阿姆哈拉语、梵语、阿拉伯语、亚美尼亚语、阿萨姆语、阿塞拜疆语等96种语言，约1.5万亿字符。\n",
    "\n",
    "ERNIE-M的学习过程由两阶段组成。第一阶段从少量的双语语料中学习跨语言理解能力，使模型学到初步的语言对齐关系；第二阶段使用回译的思想，通过大量的单语语料学习，增强模型的跨语言理解能力。\n",
    "\n",
    "[百度NLP知乎介绍](https://zhuanlan.zhihu.com/p/344810337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:12:02.784577Z",
     "iopub.status.busy": "2023-09-06T05:12:02.784066Z",
     "iopub.status.idle": "2023-09-06T05:12:03.084418Z",
     "shell.execute_reply": "2023-09-06T05:12:03.083699Z",
     "shell.execute_reply.started": "2023-09-06T05:12:02.784546Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddle.vision import models\n",
    "import paddle\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer,BertModel,ErnieViLModel,ErnieViLProcessor\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnet101'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode           \n",
    "        \n",
    "        self.fuse = BertModel.from_pretrained(\"bert-base-multilingual-cased\").encoder\n",
    "\n",
    "        ernie_vil = ErnieViLModel.from_pretrained(\"ernie_vil-2.0-base-zh\")\n",
    "        self.text_encoder = ernie_vil.text_model  \n",
    "        self.visual_encoder = ernie_vil.vision_model \n",
    "        self.processor = ErnieViLProcessor.from_pretrained(\"ernie_vil-2.0-base-zh\")\n",
    "       \n",
    "        self.fuse.layers = self.fuse.layers[:4]\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "        # self.resnet = EncoderCNN()\n",
    "        # self.projection = nn.Linear(2*(768+2048),1024)\n",
    "        self.classifier = nn.Linear(768,3)\n",
    "        # self.attention_text = nn.MultiHeadAttention(768,16)\n",
    "        # self.attention_image = nn.MultiHeadAttention(2048,16)\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768,3)\n",
    "        \n",
    "\n",
    "    def forward(self,qCap,qImg,caps,imgs):\n",
    "        encode_dict_qcap = self.processor(text = qCap ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "        input_ids_qcap = encode_dict_qcap['input_ids']\n",
    "        input_ids_qcap = paddle.to_tensor(input_ids_qcap)\n",
    "        with paddle.no_grad():\n",
    "            qcap_feature, pooled_output= self.text_encoder(input_ids_qcap) #(b,length,dim)\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:,0,:].squeeze(1))\n",
    "            return logits\n",
    "        # print(len(caps))\n",
    "        # print(caps[0])\n",
    "        # print(caps[0].shape)\n",
    "        # print(imgs.shape)\n",
    "        # print(qImg.shape)\n",
    "\n",
    "        caps_feature = []\n",
    "        with paddle.no_grad():\n",
    "            for i,caption in enumerate (caps):\n",
    "                encode_dict_cap = self.processor(text = caption ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "                input_ids_caps = encode_dict_cap['input_ids']\n",
    "                input_ids_caps = paddle.to_tensor(input_ids_caps)\n",
    "                cap_feature, pooled_output= self.text_encoder(input_ids_caps) #(b,length,dim)\n",
    "                caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature,axis=0) #(b,num,length,dim)\n",
    "        caps_feature = caps_feature.mean(axis=1)#(b,length,dim)\n",
    "\n",
    "        # caps_feature = self.attention_text(qcap_feature,caps_feature,caps_feature) #(b,length,dim)\n",
    "        \n",
    "        imgs_features = []\n",
    "        with paddle.no_grad():\n",
    "            for img in imgs:\n",
    "                imgs_feature, pooled_output= self.visual_encoder(img) #(length,dim)\n",
    "                imgs_features.append(imgs_feature)\n",
    "    \n",
    "        imgs_features = paddle.stack(imgs_features,axis=0) #(b,length,dim)\n",
    "        qImg_features = []\n",
    "        with paddle.no_grad():\n",
    "            for qImage in qImg:\n",
    "                qImg_feature, pooled_output = self.visual_encoder(qImage.unsqueeze(axis=0)) #(1,dim)\n",
    "                qImg_features.append(qImg_feature)\n",
    "        qImg_feature = paddle.stack(qImg_features,axis=0) #(b,1,dim)\n",
    "        imgs_features = imgs_features.mean(axis=1)\n",
    "        # imgs_features = self.attention_image(qImg_feature,imgs_features,imgs_features) #(b,1,dim)\n",
    "        # b,n,l,dim = imgs_features.mean(axis=1)\n",
    "        # print(len(qcap_feature))\n",
    "        # print(qcap_feature[0].shape)\n",
    "        # print(caps_feature.shape)\n",
    "        # print(qImg_feature.shape)\n",
    "        # print(imgs_features.shape)\n",
    "\n",
    "\n",
    "        # [1, 128, 768] [1, 128, 768] [1, 1, 2048] [1, 1, 2048] origin\n",
    "        # print(qcap_feature.shape,caps_feature.shape,qImg_feature.shape,imgs_features.shape)\n",
    "        # print((qcap_feature[:,0,:].shape,caps_feature[:,0,:].shape,qImg_feature.squeeze(1).shape,imgs_features.squeeze(1).shape))\n",
    "        # ([1,768], [1 , 768], [1, 2048], [1,  2048])\n",
    "        # feature = paddle.concat(x=[qcap_feature, paddle.reshape(caps_feature,[b,-1,dim]), qImg_feature.squeeze(1), paddle.reshape(imgs_features,[b,-1,dim])], axis=1) \n",
    "        feature = paddle.concat(x=[qcap_feature, caps_feature, qImg_feature.squeeze(1), imgs_features], axis=1) \n",
    "        feature = self.fuse(feature).mean(axis=1)\n",
    "        feature=self.norm(feature)\n",
    "        logits = self.classifier(feature)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:12:03.085934Z",
     "iopub.status.busy": "2023-09-06T05:12:03.085377Z",
     "iopub.status.idle": "2023-09-06T05:12:07.738745Z",
     "shell.execute_reply": "2023-09-06T05:12:07.737875Z",
     "shell.execute_reply.started": "2023-09-06T05:12:03.085910Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-09-06 19:28:14,305] [    INFO]\u001b[0m - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-09-06 19:28:15,758] [ WARNING]\u001b[0m - Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.layer_norm.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[32m[2023-09-06 19:28:15,759] [    INFO]\u001b[0m - All the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "configuration file<config.json> or <model_config.json> not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25912\\2339282770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 声明模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetWork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25912\\3110572088.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-multilingual-cased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mernie_vil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErnieViLModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ernie_vil-2.0-base-zh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mernie_vil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisual_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mernie_vil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvision_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\model_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, from_hf_hub, subfolder, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mSaves\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrelated\u001b[0m \u001b[0mresources\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0munder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msaved\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mnamed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[1;34m\"model_config.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msaved\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mnamed\u001b[0m \u001b[1;34m\"model_state.pdparams\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\model_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained_v2\u001b[1;34m(cls, pretrained_model_name_or_path, from_hf_hub, subfolder, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, from_hf_hub, cache_dir, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\Software\\CodeApp\\Anaconda\\envs\\paddle\\lib\\site-packages\\paddlenlp\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: configuration file<config.json> or <model_config.json> not found"
     ]
    }
   ],
   "source": [
    "# 声明模型\n",
    "model = NetWork(\"image\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:12:07.740512Z",
     "iopub.status.busy": "2023-09-06T05:12:07.740169Z",
     "iopub.status.idle": "2023-09-06T05:12:07.752235Z",
     "shell.execute_reply": "2023-09-06T05:12:07.751611Z",
     "shell.execute_reply.started": "2023-09-06T05:12:07.740479Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875 87\r\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "print(num_training_steps,warmup_steps)\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(1e-4, num_training_steps, warmup_steps)\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir =\"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "# 创建保存的文件夹\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(best_dir,exist_ok=True)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:12:07.753407Z",
     "iopub.status.busy": "2023-09-06T05:12:07.753090Z",
     "iopub.status.idle": "2023-09-06T05:12:07.758894Z",
     "shell.execute_reply": "2023-09-06T05:12:07.758290Z",
     "shell.execute_reply.started": "2023-09-06T05:12:07.753387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义线下评估 评价指标为acc 线上评估是f1score\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:      \n",
    "        labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "        logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:12:07.762415Z",
     "iopub.status.busy": "2023-09-06T05:12:07.762055Z",
     "iopub.status.idle": "2023-09-06T09:30:45.458188Z",
     "shell.execute_reply": "2023-09-06T09:30:45.457132Z",
     "shell.execute_reply.started": "2023-09-06T05:12:07.762394Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train run start\r\n",
      "global step 25, epoch: 1, batch: 25, loss: 0.94669, accu: 0.43812, speed: 0.02 step/s\r\n",
      "image file is truncated (63 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1400/2443.jpg\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/539/2272.jpg\r\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.66914, accu: 0.52594, speed: 0.03 step/s\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/37/6068.jpg\r\n",
      "global step 75, epoch: 1, batch: 75, loss: 0.80586, accu: 0.58771, speed: 0.03 step/s\r\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.47756, accu: 0.62328, speed: 0.02 step/s\r\n",
      "global step 125, epoch: 1, batch: 125, loss: 0.71993, accu: 0.64975, speed: 0.03 step/s\r\n",
      "image file is truncated (36 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1512/11876.jpg\r\n",
      "image file is truncated (27 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/3465/13511.jpg\r\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.40399, accu: 0.67125, speed: 0.02 step/s\r\n",
      "global step 175, epoch: 1, batch: 175, loss: 0.56741, accu: 0.69099, speed: 0.02 step/s\r\n",
      "eval loss: 1.27304, accu: 0.68602\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/539/2272.jpg\r\n",
      "global step 200, epoch: 2, batch: 25, loss: 0.57078, accu: 0.79312, speed: 0.01 step/s\r\n",
      "global step 225, epoch: 2, batch: 50, loss: 0.33577, accu: 0.82188, speed: 0.02 step/s\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/37/6068.jpg\r\n",
      "image file is truncated (63 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1400/2443.jpg\r\n",
      "global step 250, epoch: 2, batch: 75, loss: 0.38339, accu: 0.83313, speed: 0.02 step/s\r\n",
      "image file is truncated (36 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1512/11876.jpg\r\n",
      "global step 275, epoch: 2, batch: 100, loss: 0.44711, accu: 0.83891, speed: 0.02 step/s\r\n",
      "global step 300, epoch: 2, batch: 125, loss: 0.40034, accu: 0.84437, speed: 0.03 step/s\r\n",
      "global step 325, epoch: 2, batch: 150, loss: 0.43621, accu: 0.84531, speed: 0.03 step/s\r\n",
      "image file is truncated (27 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/3465/13511.jpg\r\n",
      "global step 350, epoch: 2, batch: 175, loss: 0.34733, accu: 0.84862, speed: 0.03 step/s\r\n",
      "eval loss: 1.13823, accu: 0.69290\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/539/2272.jpg\r\n",
      "image file is truncated (63 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1400/2443.jpg\r\n",
      "global step 375, epoch: 3, batch: 25, loss: 0.37484, accu: 0.84625, speed: 0.01 step/s\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/37/6068.jpg\r\n",
      "global step 400, epoch: 3, batch: 50, loss: 0.36096, accu: 0.86187, speed: 0.02 step/s\r\n",
      "image file is truncated (27 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/3465/13511.jpg\r\n",
      "global step 425, epoch: 3, batch: 75, loss: 0.21649, accu: 0.86792, speed: 0.03 step/s\r\n",
      "global step 450, epoch: 3, batch: 100, loss: 0.25992, accu: 0.87062, speed: 0.02 step/s\r\n",
      "image file is truncated (36 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1512/11876.jpg\r\n",
      "global step 475, epoch: 3, batch: 125, loss: 0.25663, accu: 0.87262, speed: 0.02 step/s\r\n",
      "global step 500, epoch: 3, batch: 150, loss: 0.41396, accu: 0.87271, speed: 0.02 step/s\r\n",
      "global step 525, epoch: 3, batch: 175, loss: 0.23382, accu: 0.87294, speed: 0.03 step/s\r\n",
      "eval loss: 1.18289, accu: 0.72422\r\n",
      "image file is truncated (36 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1512/11876.jpg\r\n",
      "global step 550, epoch: 4, batch: 25, loss: 0.28846, accu: 0.89938, speed: 0.01 step/s\r\n",
      "image file is truncated (27 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/3465/13511.jpg\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/539/2272.jpg\r\n",
      "global step 575, epoch: 4, batch: 50, loss: 0.13231, accu: 0.89875, speed: 0.02 step/s\r\n",
      "global step 600, epoch: 4, batch: 75, loss: 0.21116, accu: 0.90187, speed: 0.03 step/s\r\n",
      "global step 625, epoch: 4, batch: 100, loss: 0.40082, accu: 0.90500, speed: 0.03 step/s\r\n",
      "global step 650, epoch: 4, batch: 125, loss: 0.35987, accu: 0.90500, speed: 0.03 step/s\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/37/6068.jpg\r\n",
      "global step 675, epoch: 4, batch: 150, loss: 0.26251, accu: 0.90760, speed: 0.02 step/s\r\n",
      "image file is truncated (63 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1400/2443.jpg\r\n",
      "global step 700, epoch: 4, batch: 175, loss: 0.25360, accu: 0.90862, speed: 0.03 step/s\r\n",
      "eval loss: 1.48625, accu: 0.68831\r\n",
      "image file is truncated (36 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1512/11876.jpg\r\n",
      "global step 725, epoch: 5, batch: 25, loss: 0.12931, accu: 0.92250, speed: 0.01 step/s\r\n",
      "image file is truncated (63 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/1400/2443.jpg\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/37/6068.jpg\r\n",
      "global step 750, epoch: 5, batch: 50, loss: 0.45460, accu: 0.92531, speed: 0.02 step/s\r\n",
      "image file is truncated (8 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/539/2272.jpg\r\n",
      "global step 775, epoch: 5, batch: 75, loss: 0.19855, accu: 0.92375, speed: 0.03 step/s\r\n",
      "image file is truncated (27 bytes not processed)\r\n",
      "/home/aistudio/data/data229919/queries_dataset_merge/train/img_html_news/3465/13511.jpg\r\n",
      "global step 800, epoch: 5, batch: 100, loss: 0.16219, accu: 0.92297, speed: 0.03 step/s\r\n",
      "global step 825, epoch: 5, batch: 125, loss: 0.10128, accu: 0.92275, speed: 0.02 step/s\r\n",
      "global step 850, epoch: 5, batch: 150, loss: 0.33610, accu: 0.92448, speed: 0.02 step/s\r\n",
      "global step 875, epoch: 5, batch: 175, loss: 0.15511, accu: 0.92471, speed: 0.03 step/s\r\n",
      "eval loss: 1.49180, accu: 0.67685\r\n"
     ]
    }
   ],
   "source": [
    "# 定义训练\n",
    "def do_train(model, criterion, metric, val_dataloader,train_dataloader):\n",
    "    print(\"train run start\")\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    best_accuracy=0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_dataloader, start=1):\n",
    "            labels, cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "            probs = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "            loss = criterion(probs, labels)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1 \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            # 每间隔 1 step 输出训练指标\n",
    "            if global_step % 25 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            # 每间隔一个epoch 在验证集进行评估\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss,eval_accu=evaluate(model, criterion, metric, val_dataloader)\n",
    "                save_param_path = os.path.join(save_dir+str(epoch), 'model_state.pdparams')\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "                if(best_accuracy<eval_accu):\n",
    "                    best_accuracy=eval_accu\n",
    "                    # 保存模型\n",
    "                    save_param_path = os.path.join(best_dir, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "do_train(model, criterion, metric, val_dataloader,train_dataloader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、模型预测\n",
    "**模型预测前，请重启内核，清空占用的显存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T09:30:45.460258Z",
     "iopub.status.busy": "2023-09-06T09:30:45.459832Z",
     "iopub.status.idle": "2023-09-06T09:30:47.161327Z",
     "shell.execute_reply": "2023-09-06T09:30:47.160609Z",
     "shell.execute_reply.started": "2023-09-06T09:30:45.460215Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from best_model/model_best.pdparams\r\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'best_model/model_best.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T09:30:47.162608Z",
     "iopub.status.busy": "2023-09-06T09:30:47.162292Z",
     "iopub.status.idle": "2023-09-06T09:34:45.442381Z",
     "shell.execute_reply": "2023-09-06T09:34:45.441547Z",
     "shell.execute_reply.started": "2023-09-06T09:30:47.162585Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [03:57<00:00,  6.60s/it]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 1, 2]\r\n",
      "1129\r\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "id2name ={ 0:\"non-rumor\", 1:\"rumor\",2:\"unverified\"}\n",
    "model.eval()\n",
    "count=0\n",
    "bar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "for batch in bar:\n",
    "    count+=1\n",
    "    cap_batch, img_batch, qCap_batch, qImg_batch = batch\n",
    "    logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    label = paddle.argmax(probs, axis=1).numpy()\n",
    "    results += label.tolist()\n",
    "\n",
    "print(results[:5])\n",
    "print(len(results))\n",
    "results = [id2name[i] for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T09:34:45.443985Z",
     "iopub.status.busy": "2023-09-06T09:34:45.443628Z",
     "iopub.status.idle": "2023-09-06T09:34:45.455975Z",
     "shell.execute_reply": "2023-09-06T09:34:45.455345Z",
     "shell.execute_reply.started": "2023-09-06T09:34:45.443955Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 1129)\r\n"
     ]
    }
   ],
   "source": [
    "# 输出结果\n",
    "import pandas as pd\n",
    "#id/label\n",
    "#字典中的key值即为csv中的列名\n",
    "id_list=range(len(results))\n",
    "print(id_list)\n",
    "frame = pd.DataFrame({'id':id_list,'label':results})\n",
    "frame.to_csv(\"result.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T09:34:45.457099Z",
     "iopub.status.busy": "2023-09-06T09:34:45.456811Z",
     "iopub.status.idle": "2023-09-06T09:34:45.967419Z",
     "shell.execute_reply": "2023-09-06T09:34:45.966585Z",
     "shell.execute_reply.started": "2023-09-06T09:34:45.457078Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result.csv (deflated 81%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip test.zip result.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九、后续优化\n",
    "\n",
    "baseline分数只有65分，还有很大的改进地方，大家多多尝试，下面是一些想法\n",
    "\n",
    "参数调优：学习率、优化器以及其他超参数等\n",
    "\n",
    "特征提取：更换预训练权重更大的图像特征提取器or文本特征提取器（Ernie or Bert系列）\n",
    "\n",
    "特征交互：目前使用多头自注意力机制对文本与文本证据交互、图像与图像证据交互，可以尝试文本与图像之间的跨模态交互\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
